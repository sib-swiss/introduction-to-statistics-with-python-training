{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content <a id='toc'></a>\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1. Fisher's exact test and the Chi-square test](#0)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.1 Fisher's exact test](#1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.2 Chi-square](#2)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Exercise 01](#3)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2. Kolmogorov-Smirnov test](#4)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Exercise 02](#5)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3. Bartlett's test - testing variance equality](#6)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4. 1-way anova](#7)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Exercise 03](#8)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[5. Common probability distributions](#9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### This is some configuration to make the plots work better when presenting online\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 10, 10\n",
    "plt.rc(\"font\", size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen the basis of statistical hypothesis testing, let's review some of the most used ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# 1. Fisher's exact test and the Chi-square test  <a id='0'></a>\n",
    "\n",
    "These two tests have for object the association between 2 categorical variables.\n",
    "\n",
    "Their **null hypothesis** is the absence of association between the two variable.\n",
    "\n",
    "\n",
    "**Fisher's exact test**, as its name entails, computes a p-value which is exact, even for very low sample sizes. However it becomes computationally complex to compute as the data set size or number of categories gets high.\n",
    "\n",
    "The **Chi-square test**, in contrast, uses an approximation of the exact p-value which is only valid when samples are big enough. However, it scales well to larger samples sizes and number of categories.\n",
    "\n",
    "\n",
    "Both tests start from a **contingency table**.\n",
    "\n",
    "We are going to use as example the historical [Lady tasting tea](https://en.wikipedia.org/wiki/Lady_tasting_tea).\n",
    "\n",
    "|  | detected as milk before | detected as milk after | marginal sums |\n",
    "|---|---|---|---|\n",
    "| **milk before** | 3 | 1 | **4** |\n",
    "| **milk after** | 1 | 3 | **4** |\n",
    "| **marginal sums**  | **4** | **4** | **8** |\n",
    "\n",
    "In our experiment, the lady was able to correctly identify 6 out of 8 cups.\n",
    "\n",
    "\n",
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.1 Fisher's exact test  <a id='1'></a>\n",
    "\n",
    "The test is based on counting the number of possible tables that show the same maginal sums.\n",
    "\n",
    "The p-value corresponds to the number of tables \n",
    "as or more extreme as the observed one,\n",
    "divided by the total number of tables.\n",
    "\n",
    "Given the constraint of keeping the marginal sums, describing the \n",
    "number of *correctly detected cups with the milk before* suffices to describe a type of table.\n",
    "\n",
    "\n",
    "In our case, tables as or more extreme as the observed ones are the one where the number of correctly detected cups with the milk before is :\n",
    "* 3 : 16 tables :\n",
    "     * 4 ways of selecting 3 cups with milk before correctly\n",
    "     * 4 ways of selecting 3 cups with milk after correctly\n",
    "* 1 : 16 tables :\n",
    "     * 4 ways of selecting 1 cups with milk before correctly\n",
    "     * 4 ways of selecting 1 cups with milk after correctly\n",
    "* 4 : 1 table (1x1)\n",
    "* 0 : 1 table (1x1)\n",
    "\n",
    "\n",
    "Given that there is 70 possible tables here, the p-value is $(16+1+1+16)/70 \\approx 0.486$\n",
    "\n",
    "> Note : by defining more extreme tables as the ones where there is as many or more successes than observed, we would effectively be performing a 1-sided test.\n",
    "\n",
    "While it is fun to use combinatorics to compute p-values, scipy has a function for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [[3,1],[1,3]]\n",
    "\n",
    "oddsratio , pvalue = stats.fisher_exact(table)\n",
    "print(\"Fisher's exact test\")\n",
    "print('\\todds ratio:',oddsratio)\n",
    "print('\\tp-value:',pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds ratio corresponds to the product of the row-wise odds (i.e., number correct guess divided by number of wrong guesses):\n",
    "\n",
    "$$\\frac{3}{1} * \\frac{3}{1} = 9$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.2 Chi-square  <a id='2'></a>\n",
    "\n",
    "The Chi-square test is based on an approximation, \n",
    "which works best when the expected **number of observations in each cell exceeds 5**.\n",
    "\n",
    "Nevertheless, we can still compute the test statistic for our *simple* example.\n",
    "\n",
    "The idea of the test is that under the null hypothesis that the two variables are not linked, the expected values in each of the cells of the table can be deduced from the marginal sums only.\n",
    "\n",
    "In our case that gives the folowwing expected table:\n",
    "\n",
    "|  | detected as milk before | detected as milk after | marginal sums |\n",
    "|---|---|---|---|\n",
    "| **milk before** | 8x(4/8)x(4/8)=2 | 8x(4/8)x(4/8)=2 | **4** |\n",
    "| **milk after** | 8x(4/8)x(4/8)=2 | 8x(4/8)x(4/8)=2 | **4** |\n",
    "| **marginal sums**  | **4** | **4** | **8** |\n",
    "\n",
    "> Sure, our example is a bit boring here.\n",
    "\n",
    "Then, the test statistic of the test :\n",
    "\n",
    "$$T = \\sum \\frac{(observed-expected)^2}{expected}$$\n",
    "\n",
    "For our example, \n",
    "\n",
    "$$ T = \\frac{(3-2)^2}{2} +\\frac{(1-2)^2}{2} + \\frac{(1-2)^2}{2} +\\frac{(3-2)^2}{2} = 2$$\n",
    "\n",
    "\n",
    "Which is expected to follow a $\\chi^2$ (chi-square) distribution with a number of degree of freedom equal to:\n",
    "\n",
    "$$ df = (number\\_of\\_columns - 1) * (number\\_of\\_rows - 1) $$\n",
    "\n",
    "In our case $df=(2-1)*(2-1)=1$\n",
    "\n",
    "> Note: this is directly related to the Fisher's exact test where we could describe tables using a single value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2,pval , df, expected = stats.chi2_contingency(table , correction=False)\n",
    "print(\"Chi-square test\")\n",
    "print('\\tchi2:', chi2)\n",
    "print('\\tp-value:', pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the returned p-value is quite different from the one given by Fisher's exact test.\n",
    "\n",
    "> note that here we use `correction=False` as by default scipy implementation uses [Yates's correction](https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity), which is useful when the effectives are low. Try the same lines with the correction to see the difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine now that we have a many cups and very patient lady so that the contigency table looks like this:\n",
    "\n",
    "|  | detected as milk before | detected as milk after | marginal sums |\n",
    "|---|---|---|---|\n",
    "| **milk before** | 25 | 15 | **40** |\n",
    "| **milk after** | 18 | 22 | **40** |\n",
    "| **marginal sums**  | **40** | **40** | **80** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [[25,15],[18,22]]\n",
    "\n",
    "oddsratio , pvalue = stats.fisher_exact(table)\n",
    "print(\"Fisher's exact test\")\n",
    "print('\\todds ratio:',oddsratio)\n",
    "print('\\tp-value:',pvalue)\n",
    "\n",
    "chi2,pval , df, expected = stats.chi2_contingency(table , correction=False)\n",
    "print(\"Chi-square test\")\n",
    "print('\\tchi2:', chi2)\n",
    "print('\\tp-value:', pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the p-value of the Chi-square test is now much closer to that of Fisher's exact test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise 01  <a id='3'></a>\n",
    "\n",
    "Come back to the census data from 1880, in particular the `'data/census1880_fractions.csv'` file we saved.\n",
    "\n",
    "1. Test the association between majority religion (`'majority_religion'`) and majority language (`'majority_language'`).\n",
    "\n",
    "> Tip: to create a contingency table :\n",
    "\n",
    "> ```table = pd.crosstab( dfFractions['majority religion'] , dfFractions['majority language'] )```\n",
    "\n",
    "\n",
    "2. How could you make Fisher's test work here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFractions = pd.read_csv('data/census1880_fractions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2. Kolmogorov-Smirnov test  <a id='4'></a>\n",
    "\n",
    "The Kolmogorov-Smirnov is a **nonparametric test that compares entire distributions**.\n",
    "It can either be used to compare the distribution of samples, or the distribution of a single sample with a distribution of reference.\n",
    "\n",
    "Contrary to the t-test which is only a test of location, the **KS test also differentiate differences in scale and shape**.\n",
    "\n",
    "The statistic of the Kolmogorov-smirnov test corresponds to the maximal distance between the cumulative distribution functions of the sample and the reference distribution (1-sample test) or the second sample (2-sample test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampleSize=1000\n",
    "sample = np.random.randn(sampleSize)*0.5 + 1 -2* (np.random.random(sampleSize)>0.5) \n",
    "\n",
    "# computing the observed CDF and comparing with what is expected under a normal distribution\n",
    "cdfobserved = np.arange(1,len(sample)+1)/len(sample)\n",
    "sample.sort()\n",
    "cdfexpected = stats.norm.cdf(sample)\n",
    "\n",
    "diff = abs(cdfexpected-cdfobserved)\n",
    "positionMax = np.argmax(diff) \n",
    "maxDiff = diff[positionMax]\n",
    "maxPos = sample[positionMax]\n",
    "\n",
    "# plotting the pdf and cdf \n",
    "x = np.linspace(-3,3,100)\n",
    "fig,axes = plt.subplots(1,2,figsize=(14,5))\n",
    "sns.histplot( sample , label='sampled data' , stat='density', kde=True, ax = axes[0])\n",
    "sns.lineplot( x=x , y=stats.norm.pdf(x) , color='xkcd:orange' , label='expected' , ax = axes[0])\n",
    "          \n",
    "sns.histplot(sample, bins = len(sample), stat='density', \n",
    "             cumulative=True, label='sampled data' , ax = axes[1] ).set_zorder(1)\n",
    "sns.lineplot( x=x , y=stats.norm.cdf(x) , color='xkcd:orange' , label='expected' , ax = axes[1])\n",
    "#adding a segment to visualize the KS statistic\n",
    "axes[1].plot( [ maxPos , maxPos ] , [ cdfobserved[positionMax] , cdfexpected[positionMax] ] ,color='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test :\n",
    "ksStat , pvalue = stats.kstest( sample , stats.norm.cdf )\n",
    "print('1-sample Kolmogorov-Smirnov test:')\n",
    "print( 'KS test statistic :',ksStat )\n",
    "print( 'p-value :',pvalue )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for 2 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize=100\n",
    "sample1 = np.random.randn(sampleSize)*1 \n",
    "sample2 = np.random.randn(sampleSize)*0.6 + 1.0\n",
    "\n",
    "# plotting the pdf and cdf \n",
    "fig,axes = plt.subplots(1,2,figsize=(14,5))\n",
    "sns.histplot( sample1 , label='sample1' ,binwidth=0.2, kde=True , ax = axes[0])\n",
    "sns.histplot( sample2 , label='sample2' ,binwidth=0.2,  color='xkcd:orange' , kde=True , ax = axes[0])\n",
    "axes[0].legend()\n",
    "kwargs = {'cumulative': True}\n",
    "sns.histplot(sample1, bins = len(sample), stat='density', \n",
    "             cumulative=True, kde=False, label='sample1' , ax = axes[1] )\n",
    "sns.histplot(sample2, bins = len(sample), stat='density', \n",
    "             cumulative=True,  color='xkcd:orange', kde=False, label='sample2' , ax = axes[1] )\n",
    "\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test :\n",
    "ksStat , pvalue = stats.ks_2samp(sample1,sample2)\n",
    "print('1-sample Kolmogorov-Smirnov test:')\n",
    "print( 'KS test statistic :',ksStat )\n",
    "print( 'p-value :',pvalue )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kolmogorov-Smirnov is very useful because it is non parametric (i.e., less assumptions to check) and it accounts for variations in general and not only in location.\n",
    "\n",
    "The reason we keep using the t-test when we want to compare locations is because the **KS test has a worse statistical power than the t-test**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize=10\n",
    "diff = 1.0\n",
    "sigThreshold=0.05\n",
    "N=1000\n",
    "\n",
    "rejectedKS = 0\n",
    "rejectedT = 0\n",
    "\n",
    "for i in range(N):\n",
    "    sample1 = np.random.randn(sampleSize) \n",
    "    sample2 = np.random.randn(sampleSize) + diff\n",
    "    \n",
    "    ## is the KS test able to find the difference ?\n",
    "    ksStat , pvalue = stats.ks_2samp(sample1,sample2)\n",
    "    if pvalue <= sigThreshold:\n",
    "        rejectedKS+=1\n",
    "    \n",
    "    ## is the t-test able to find the difference ?\n",
    "    tstat , pvalue = stats.ttest_ind(sample1,sample2)\n",
    "    if pvalue <= sigThreshold:\n",
    "        rejectedT+=1\n",
    "\n",
    "print(\"Power for a difference in mean\",diff, ', sample size',sampleSize,'and significance threshold',sigThreshold)\n",
    "print('KS test',rejectedKS/N)\n",
    "print('T test ',rejectedT /N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the case as well with tests of normality such as the Shapiro-Wilk test.\n",
    "\n",
    "> You can think of it as the KS test being more generalistic, and performing a bit worse than specialised tests in their respective areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise 02   <a id='5'></a>\n",
    "\n",
    "In a previous exercise we used the t-test to detect a difference in the weight of mice subjected to different diets. This dataset presents another condition : `'genotype'`\n",
    "\n",
    "Use the Kolmogorov-Smirnov test to determine if the distributions of mice weights differ between wild-type (`'WT'`) and mutant (`'KO'`) individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_data = pd.read_csv( 'data/mice_data.csv' ) # data about the weight of mices of different genotypes and subjected to different diets\n",
    "sns.catplot(x='weight' , y='genotype' , data=mice_data , kind='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 3. Bartlett's test - testing variance equality  <a id='6'></a>\n",
    "\n",
    "We have seen with the t-test how to check for mean equality. \n",
    "There also exists tests to test the equality of variance between different samples (also refered to as *homoscedasticity*).\n",
    "Bartlett's test is one such test.\n",
    "\n",
    "**assumptions:** Bartlett's presumes that the data is normally distributed (remember Shapiro-wilk's test in the previous notebook).\n",
    "\n",
    "**Test hypothesis**: given $m$ groups of variances $\\sigma^2_{1...m}$, , containing $n_1...m$ observations (for a total of $n$)\n",
    " * **Null hypothesis**: $H_0 = \\sigma^2_1 = \\sigma^2_2 = ... = \\sigma^2_m$\n",
    " * **Alternative hypothesis**: At least one of these variance differ from the others\n",
    "\n",
    "**Test statistic**\n",
    "\n",
    "$$T = \\frac{(n-m) ln(s^2_{pop}) - \\sum (n_i-1)ln( s^2_i )}{1+\\frac{1}{3(m-1)} ( \\sum  \\frac{1}{n_i-1}  - \\frac{1}{n-m} ) }$$\n",
    "\n",
    "Where\n",
    "\n",
    "* $s^2_{i} = \\frac{1}{n_i -1} \\sum(x_i - \\bar{x_i})^2$ is the *sample variance* \n",
    "* $s^2_{pop} = \\sum \\frac{n_i - 1}{n-m} s^2_i $ is the *pooled variance* estimate\n",
    "\n",
    "Under the null hypothesis, the test statistic $T$ approximately follows a $\\chi^2$ distribution with $m-1$ degrees of liberty.\n",
    "\n",
    "\n",
    "Let's test this approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = [] # will contain a sample of values of T under the null hypothesis\n",
    "ni = 3\n",
    "m=3\n",
    "for i in range(10000):\n",
    "    samples = [ np.random.randn(ni) for i in range(m)]\n",
    "    stat, p = stats.bartlett(*samples)\n",
    "    T.append(stat)\n",
    "\n",
    "#performing a KS test to check if the test statistic follows the expected chi-square distribution\n",
    "ksStat , pvalue = stats.kstest( T , lambda x : stats.chi2.cdf(x,df=m-1) )\n",
    "print('Does T significantly differ from a Chi square distribution?')\n",
    "print('\\tp-value of KS test :',pvalue)\n",
    "sns.displot(T , color='xkcd:avocado' , stat='density' , aspect=3, label=\"sampled Bartlett's test statistic\")\n",
    "x = np.linspace( 0,20,100 )\n",
    "sns.lineplot( x=x , y=stats.chi2.pdf(x,df=m-1) , color='xkcd:tomato', linewidth=3 , label=\"expected Chi-square\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation seems quite valid even when $n_i$ or $m$ are low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data cannot be considered normal, other tests can be used such as the **Levene test** (see [`scipy.stats.levene`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.levene.html#scipy.stats.levene))\n",
    "\n",
    "\n",
    "> Note : no exercise here, but don't be afraid : you will get to apply Bartlett's test in the next exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 4. 1-way anova  <a id='7'></a>\n",
    "\n",
    "The ANOVA, or ANalyse Of VAriance, stands maybe among the most used (and abused) type of statistical tests to date.\n",
    "\n",
    "The anova is used to analyze the differences among group means in a sample. \n",
    "In particular, we are going to concentrate here on the 1-way ANOVA, which evaluates the difference in means of a numerical variable across groups formed by another (single) variable.\n",
    "\n",
    "In this sense, it is a generalization of the t-test which is limited to 2 groups only (in fact, the 1-way anova and t-test are quivalent when there are only 2 groups).\n",
    "\n",
    "**Anova assumptions** :\n",
    "* subpopulation distributions are normal\n",
    "* samples have equal variances\n",
    "* observations are independent from one another\n",
    "\n",
    "**Test hypothesis** : \n",
    "given $m$ groups of mean $\\bar{x}_{1...m}$, each containing $n_i$ observations (for a total of $n$)\n",
    " * **Null hypothesis** : $H_0 : \\bar{x}_1 = \\bar{x}_2 = ... = \\bar{x}_m$\n",
    " * **Alternative hypothesis** : At least one of these means differ from the others\n",
    " \n",
    "The anova relies on the idea that if the mean varies between the different group then the overall variance of all samples should be significantly greater than the variance within each group (hence the name).\n",
    "\n",
    "Put a bit more formally, the anova is interested in the **sum of squared** differences with the mean (abbreviated SSq)\n",
    "\n",
    "> remember that a variance is a normalised sum of squared difference with the mean\n",
    "\n",
    "\n",
    "It relies on the decomposition :\n",
    "\n",
    "$$ SS_{total} = SS_{within} + SS_{between} $$\n",
    "\n",
    "Where:\n",
    "* $ SS_{total} = \\sum (x_{ij}-\\bar{x})^2 $ is the sum of the SSq within the full data\n",
    "* $ SS_{within} = \\sum (x_{ij}-\\bar{x_{i}})^2 $ is the sum of the SSq within each groups\n",
    "* $ SS_{between} = \\sum n_i * (\\bar{x_{i}}-\\bar{x})^2 $ is the SSq between groups\n",
    "\n",
    "If $SS_{between} >> SS_{within}$ then the grouping explain a significant part of the total variance.\n",
    "\n",
    "The **test statistic** is:\n",
    "\n",
    "$$ F = \\frac{ SS_{between} / (m-1) }{ SS_{within} / (n-m) } $$\n",
    "\n",
    "$(m-1)$ and $(n-m)$ corresponds to the **degrees of freedom**, repectively of the between and within group sum of squares.\n",
    "\n",
    "Under the null hypothesis the test statistic follows an F-distribution with $(m-1)$ and $(n-m)$ degrees of freedom.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have measures of the height of the same number of individuals in 3 plant subspecies\n",
    "\n",
    "dfPlant = pd.read_table(\"data/Mendaxophytae_data_oct2020.csv\",sep=',')\n",
    "fig,axes = plt.subplots(1,2 , figsize=(14,7),  sharey=True )\n",
    "sns.histplot(y=dfPlant['plantSize'] , kde=True ,  ax = axes[0] )\n",
    "sns.rugplot(y=dfPlant['plantSize']  ,  ax = axes[0] )\n",
    "sns.violinplot(x='subSpecies', y='plantSize' , data=dfPlant, kind = 'violin' , ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the global mean \n",
    "grandMean = dfPlant['plantSize'].mean()\n",
    "\n",
    "#compute the mean, group size and sum of square inside each group:\n",
    "X = dfPlant.groupby('subSpecies')['plantSize'].agg(['mean','count'])\n",
    "X['SumSquare'] = dfPlant.groupby('subSpecies')['plantSize'].apply( lambda  x : sum((x-x.mean())**2) )\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the total sum of square :\n",
    "SumSqTotal = sum( (dfPlant['plantSize']-grandMean)**2 )\n",
    "\n",
    "# computing the sum of square between groups :\n",
    "SumSqBetween = sum( ( X['mean'] - grandMean )**2 * X['count'] )\n",
    "\n",
    "# computing the sum of square within groups :\n",
    "SumSqWithin = sum( X['SumSquare'] )\n",
    "\n",
    "print('sum of squares:')\n",
    "print('\\tTotal  :',SumSqTotal)\n",
    "print('\\tWithin :',SumSqWithin)\n",
    "print('\\tBetween:',SumSqBetween)\n",
    "\n",
    "print('\\tTotal - (Between+Within):',SumSqTotal - (SumSqBetween + SumSqWithin) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the degrees of freedom\n",
    "numberOfGroups = X.shape[0]\n",
    "DFbetween = numberOfGroups -1\n",
    "DFwithin = len(dfPlant.index) - numberOfGroups\n",
    "\n",
    "MeanSqWithin = SumSqWithin / DFwithin\n",
    "MeanSqBetween = SumSqBetween / DFbetween\n",
    "\n",
    "Fstat = MeanSqBetween/MeanSqWithin\n",
    "pval = 1-stats.f.cdf(Fstat, DFbetween , DFwithin)\n",
    "\n",
    "print('manual 1-way anova / F-test:')\n",
    "print('F-stat :',Fstat)\n",
    "print('p-value:',pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same result using `scipy.stats` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fstat , pval = stats.f_oneway( dfPlant['plantSize'][dfPlant['subSpecies'] == 0],\n",
    "                dfPlant['plantSize'][dfPlant['subSpecies'] == 1],\n",
    "                dfPlant['plantSize'][dfPlant['subSpecies'] == 2] )\n",
    "print('automated 1-way anova / F-test:')\n",
    "print('F-stat :',Fstat)\n",
    "print('p-value:',pval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "When the assumptions (normality, equality of variances) of the anova fail, you have several alternatives : \n",
    "* **normality OK, variances unequal :** \n",
    "    * **[Alexander Govern test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.alexandergovern.html#scipy.stats.alexandergovern)**\n",
    "    * **Welch's anova**, absent from scipy ; present in the recent pingouin package : [pingouin.welch_anova](https://pingouin-stats.org/build/html/generated/pingouin.welch_anova.html#pingouin.welch_anova)\n",
    "    \n",
    "* **data not normal : Kruskall-Wallis H test**, a nonparametric test that check **median** equality. See\n",
    "[scipy.stats.kruskall](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "Finally, it is important to remember that the ANOVA tests is at least one mean differ from the others.\n",
    "\n",
    "Usually you will want to know which couple of condition differ. For this we recommend [Tukey's Honestly Significant Difference test](https://scipy.github.io/devdocs/reference/generated/scipy.stats.tukey_hsd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise 03  <a id='8'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset `'stcp-Rdataset-Diet.csv'` contains information on the weights \n",
    "of 78 test subjects spread among 3 types of diets.\n",
    "\n",
    "Here is how to read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/stcp-Rdataset-Diet.csv')\n",
    "df['weightDiff'] = df['weight6weeks'] - df['pre.weight']\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the diets are associated with a different weight difference on average. \n",
    "Be careful to test the assumptions of your test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-6 solutions/solution_03_03.py\n",
    "## first a little plot to meet the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 7-25 solutions/solution_03_03.py\n",
    "# testing assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 26- solutions/solution_03_03.py\n",
    "# the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus exercise :** go back to the 1880 swiss census data. Is the main language spoken in a locality linked to the total number of inhabitant in town ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 5. Common probability distributions  <a id='9'></a>\n",
    "\n",
    "As you have seen statistical tests rely on the definition of **how a test statistic is distributed under a null hypothesis**. \n",
    "\n",
    "Here is a list of some well described distributions that can be used to model specific random processes :\n",
    "\n",
    "\n",
    "* **Bernoulli distribution** : two outcomes - 0 and 1 (e.g., a coin toss, where 0 = heads, 1 = tails). The outcomes do not have to be equally likely. \n",
    "    * $p$ represents the probability of \"success\" (getting outcome 1). \n",
    "* **Uniform distribution** : distribution over (possibly many) equally likely outcomes (e.g., rolling a fair die).\n",
    "    * $a$ : lowest possible value\n",
    "    * $b$ : highest possible value\n",
    "* **Binomial distribution** : sum of outcomes of variables following a Bernoulli distribution (e.g., toss a coin 50 times and count the number of tails). \n",
    "    * $n$ - number of trials\n",
    "    * $p$ - probability of \"success\" in each trial. \n",
    "    * Another common example: repeatedly drawing from an urn with a fixed number of white and black balls, putting back the drawn ball after each trial.\n",
    "* **Multinomial distribution** : generalization of the binomial distribution to more than two outcomes per trial. \n",
    "    * $n$ - number of trials, \n",
    "    * $p_1, \\ldots, p_k$ - success probabilities.\n",
    "* **Hypergeometric distribution** : drawing from an urn, but without replacement (so that the probability of success changes between trials). Used in genomics e.g. for gene set/overrepresentation analysis. There the \"urn\" consists of all genes, the trial is to \"draw\" a gene (typically by calling it significantly differentially expressed), and a \"success\" is declared if the gene comes from a pre-specified gene set. \n",
    "    * $n$ - number of trials\n",
    "    * $N$ - original population size\n",
    "    * $K$ - number of available \"success states\" in the population.\n",
    "* **Poisson distribution** : distribution of the count of times that something happens along a fixed amount of time. \n",
    "    * $\\lambda$ - average \"rate\". \n",
    "    * Limiting distribution of the binomial, as $n$ goes to infinity and $p$ goes to zero so that $np$ stays constant. \n",
    "* **Geometric distribution** : models the \"number of failures until the first success\". \n",
    "    * $p$ - probability of success in each trial. \n",
    "* **Negative Binomial distribution** : number of failures until $r$ successes have occurred. \n",
    "    * $p$ - probability of success in each trial\n",
    "    * $r$ - desired number of successes. \n",
    "    * Can also be obtained as the marginal distribution of a Poisson variable, where the rate is not constant but follows a gamma distribution. \n",
    "* **Normal distribution** : ubiquitous! \n",
    "    * $\\mu$ - mean\n",
    "    * $\\sigma$ - standard deviation\n",
    "* **Log-normal distribution** : a variable where the logarithm is normally distributed. \n",
    "* **Chi-square distribution** : the distribution of a sum of squares of normally distributed variables. \n",
    "\n",
    "\n",
    "For more information, see e.g. \n",
    "[https://www.johndcook.com/blog/distribution_chart/](https://www.johndcook.com/blog/distribution_chart/),  \n",
    "[https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/](https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/) or\n",
    "[http://www.math.wm.edu/~leemis/2008amstat.pdf](http://www.math.wm.edu/~leemis/2008amstat.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py38)",
   "language": "python",
   "name": "conda_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
