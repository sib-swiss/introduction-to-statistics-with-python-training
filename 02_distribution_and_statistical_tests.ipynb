{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content <a id='toc'></a>\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1. probability distributions](#0)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Exercise 01](#1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Exercise 02](#2)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2. Statistical hypothesis testing](#3)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.1 t-test : difference between two means](#4)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2 t-test assumptions and what to do when they are not respected](#5)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.1 testing normality](#6)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.2 nonparametric testing : Mann–Whitney U test](#7)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3 p-value, power and errors](#8)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3.1 errors of type I and II](#9)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3.2 P-values](#10)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3.3 power](#11)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Exercise 03](#12)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.4 BONUS : multiple testing](#13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# 1. probability distributions  <a id='0'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "### This is some configuration to make the plots work better when presenting online\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 10, 10\n",
    "plt.rc(\"font\", size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what is a probability distribution ?\n",
    "\n",
    "A probability distribution represents the different values that a *random variable* might take and associates a specific probability each of them.\n",
    "\n",
    "They are most commonly represented by plotting their **Probability mass/density function (pdf)**: horizontal axis represents possible values, vertical axis represents probabilities of obtaining these values (discrete) or the probability density at each value (continuous).\n",
    "\n",
    "An important property of probability distribution is that they sum to 1. \n",
    "This is quite apparent when plotting the **cumulative mass/density function (cdf)** of a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots( 2, 2, figsize=(14, 10) )\n",
    "\n",
    "# standard normal distribution\n",
    "x = np.linspace(-5,5, 100) # 100 equally separated points between -5 and 5\n",
    "a= sns.lineplot(x=x, y=stats.norm.pdf(x) , ax = axes[0,0])\n",
    "a.set(xlabel='value', ylabel='density')\n",
    "a.set_title('normal law \\n probability density function')\n",
    "a=sns.lineplot(x=x, y=stats.norm.cdf(x) , ax = axes[0,1])\n",
    "a.set(xlabel='value', ylabel='cumulative density')\n",
    "a.set_title('normal law \\n cumulative probability density function')\n",
    "\n",
    "# binomial distribution\n",
    "n, p = 5, 0.4 # parameters of the binomial distribution\n",
    "x = list(range(n+1)) # all integers from 0 to n, included.\n",
    "a=sns.scatterplot(x=x, y=stats.binom.pmf(x,n,p) , ax = axes[1,0])\n",
    "a.set(xlabel='value', ylabel='probability')\n",
    "a.set_title('binomial law \\n probability mass function')\n",
    "a=sns.lineplot(x=x, y=stats.binom.cdf(x,n,p) , marker=\"o\",  ax = axes[1,1])\n",
    "a.set(xlabel='value', ylabel='cumulative probability' )\n",
    "a.set_title('binomial law \\n cumulative probability mass function')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most distributions have a certain number of **parameters** which may control their overall **shape, location or scale**.\n",
    "\n",
    "For Instance the normal law has two parameters : its mean ($\\mu$) and its standard deviation ($\\sigma$), which respectively control its location and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10, 100) # 100 equally separated points between -5 and 5\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "a= sns.lineplot(x=x, y=stats.norm.pdf(x , loc = 0 , scale = 1) , ax = ax , label='mean= 0 , stdev=1')\n",
    "sns.lineplot(x=x, y=stats.norm.pdf(x , loc = -2 , scale = 1 ) , ax = ax , label='mean=-2 , stdev=1')\n",
    "sns.lineplot(x=x, y=stats.norm.pdf(x , loc = 1 , scale = 3 ) , ax = ax , label='mean= 1 , stdev=3')\n",
    "\n",
    "a.set(xlabel='value', ylabel='density')\n",
    "a.set_title('normal law - probability density function')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is of particular interest to us here is that \n",
    "these theoretical **probability distributions can be used to model \n",
    "how a particular random variable would behave under a particular hypothesis**.\n",
    "This prediction can then be compared to data measured in the world.\n",
    "\n",
    "To signify that a random variable follows a particular distribution, we use the $\\sim$ notation. So if variable $X$ follows a normal distribution of mean $m$ and standard deviation $s$, we write:\n",
    "$$X \\sim N(m , s )$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why everyone talks about the normal distribution - confidence interval\n",
    "\n",
    "It is more than likely that you have heard about the normal distribution and 95% confidence interval. You may also have, in this context, heard about the limit of 1.96, or 2 standard-deviations. \n",
    "\n",
    "This relates to a peculiar (and very convenient) property of random variables :\n",
    "\"in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution\" (wikipedia)\n",
    "\n",
    "Consider that, at its core, the mean of a sample is a sum independently drawn realizations of a random variable (normalized by the size of said sample). \n",
    "The central limit theorem then implies that **for large samples the mean of a sample of fixed size is a random variable that follows a normal distribution**.\n",
    "\n",
    "Let's demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Say we collect a sample of size 100 in an exponential distribution, and compute the mean of that sample\n",
    "sampleSize = 100\n",
    "sampling = lambda sampleSize : np.random.exponential( size = sampleSize) # I define a function for sampling\n",
    "\n",
    "sample1 = sampling(sampleSize)\n",
    "\n",
    "# let's repeat this 10 times, and keep only the means\n",
    "sampleMeans10 = [ sampling(sampleSize).mean() for i in range(10) ]\n",
    "# now let's repeat this 100 times, and keep only the means\n",
    "sampleMeans100 = [ sampling(sampleSize).mean() for i in range(100) ]\n",
    "# now let's repeat this 100 000 times, and keep only the means\n",
    "sampleMeans100000 = [ sampling(sampleSize).mean() for i in range(100000) ]\n",
    "\n",
    "\n",
    "## plotting \n",
    "fig, axes = plt.subplots(2,2,figsize=(14, 7))\n",
    "x = np.linspace(0,10, 100)\n",
    "a= sns.lineplot(x=x, y=stats.expon.pdf(x ) , ax = axes[0,0] )\n",
    "sns.rugplot( x=sample1 , ax = axes[0,0] )\n",
    "a.set_title('PDF and results of 1 sample')\n",
    "\n",
    "a=sns.histplot(sampleMeans10 , kde=True, ax = axes[0,1])\n",
    "a.set_title('distribution of the means\\nof 10 samples of size {}'.format(sampleSize))\n",
    "\n",
    "a=sns.histplot(sampleMeans100 , kde=True, ax = axes[1,0])\n",
    "a.set_title('distribution of the means\\nof 100 samples of size {}'.format(sampleSize))\n",
    "\n",
    "a=sns.histplot(sampleMeans100000 , kde=True, ax = axes[1,1])\n",
    "#a=sns.distplot(x, stats.norm.pdf( x , loc = 0.5 , norm= ) , ax = axes[1,1])\n",
    "a.set_title('distribution of the means\\nof 100 000 samples of size {}'.format(sampleSize))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, but that is just something you did with simulations, does that work on real data ?\n",
    "\n",
    "let's do the same, with the census data. \n",
    "Here we are in the special case were we have the entire population, so we know the **population mean**.\n",
    "\n",
    "By randomly choosing some cities, we produce a **sample mean**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv( \"data/census1880_fractions.csv\" )\n",
    "\n",
    "population_mean = df[\"Foreigner\"].mean()\n",
    "\n",
    "sample_mean = np.random.choice( df[\"Foreigner\"] , size=10 ).mean()\n",
    "\n",
    "print(\"population mean\",population_mean)\n",
    "print(\"sample mean\",sample_mean)\n",
    "print(\"scaled difference between population and sample mean: {:.3f}\".format(( population_mean - sample_mean )/population_mean ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the central limit theorem, these sample mean are not completely random : they are spread around the population mean according to a normal law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the distribution appear, I create a large number of samples (imagine you have a lot of polls)\n",
    "sample_means = []\n",
    "for i in range(1000):\n",
    "    sample_means.append( np.random.choice( df[\"Foreigner\"] , size=100 ).mean() )\n",
    "\n",
    "## plotting \n",
    "fig, axes = plt.subplots(2,1,figsize=(14, 7))\n",
    "x = np.linspace(0,10, 100)\n",
    "a= sns.kdeplot(df[\"Foreigner\"], ax = axes[0] , cut=0 )\n",
    "sns.rugplot( df[\"Foreigner\"] , ax = axes[0] )\n",
    "a.set_title('distribution of fraction of Foreigners')\n",
    "\n",
    "a=sns.histplot(sample_means , kde=True ,ax = axes[1])\n",
    "axes[1].axvline(population_mean , color='orange', label='population mean')\n",
    "a.set_title('distribution sample means')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this normal law:\n",
    " * the mean is equal to the **mean of the original distribution**  \n",
    " * the standard deviation that depends on the **standard deviation of the original deviation divided by the square root of $n$**.\n",
    "\n",
    "Using the notation we introduced earlier this looks like:\n",
    "\n",
    "$$\\bar{x} \\sim N(m , \\frac{s}{\\sqrt{n}} )$$\n",
    "\n",
    ", where $\\bar{x}$ is the sample mean, and $m$ $s$ respectively are the mean and the standard deviation of the population.\n",
    "\n",
    "However, note that **this only works if the sample size is large enough**. \n",
    "\n",
    "<br>\n",
    "\n",
    "The central limit theorem let's us perform inferences even when the underlying distribution is unknown, howver it is still an approximation so **when it is known, we should prefer using the actual original distribution.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanOriginal = 1 # the mean of this exponential law is 1\n",
    "stdevOriginal= 1 # its standard deviation is 1\n",
    "\n",
    "fig, axes = plt.subplots(3,1,figsize=(15, 10) , sharex=True)\n",
    "# we make the sample size 5 , 50 and 500 to see the resulting distribution of sample means\n",
    "for i,sampleSize in enumerate([5,50,500]): \n",
    "\n",
    "    mu = meanOriginal\n",
    "    stdev = stdevOriginal / np.sqrt(sampleSize) # expected standard deviation according to Central Limit Theorem\n",
    "\n",
    "    ax = axes[i]\n",
    "    \n",
    "    sns.histplot([ sampling(sampleSize).mean() for i in range(10000) ] , \n",
    "                 kde=True , ax = ax , stat = \"density\",\n",
    "                 label = '10 000 sampled means' )\n",
    "    x = np.linspace(0,2, 100)\n",
    "    sns.lineplot(x=x, y=stats.norm.pdf(x , loc = 1 , scale = stdev  ) ,\n",
    "                 color='orange',\n",
    "                 ax = ax , label = 'theoretical normal law' )\n",
    "\n",
    "    ax.set_title('sample size '+str(sampleSize))\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise 01  <a id='1'></a>\n",
    "\n",
    "Consider an experiment where we toss a (fair) coin.\n",
    "We repeat the experiment 10 times (sample size of 10), and compute the mean frequency at which head appear.\n",
    "\n",
    "\n",
    "The expected mean here would be 0.5 (the probability of getting head for a fair coin).\n",
    "\n",
    "The standard deviation would be `np.sqrt(0.5 * (1-0.5) ) = 0.5`\n",
    "\n",
    "\n",
    "1. What would be the normal law followed by the mean of this sample of size 10, according to the CLT ?\n",
    "2. Do you think the sample is large enough to use a normal law here ?\n",
    "\n",
    "> Note, this sum of 10 coin tosses is equivalent to a binomial law with n=10 and p=0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can perform experiments using:\n",
    "def samplingMean(n=10,p=0.5):\n",
    "    return sum( np.random.random(n)>p ) / n\n",
    "\n",
    "print( 'mean of a sample of size 10:', samplingMean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-5 solutions/solution_02_01.py\n",
    "# 1. What would be the normal law followed by the mean of this sample of size 10, according to the CLT ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 6- solutions/solution_02_01.py\n",
    "# 2. Do you think the sample is large enough to use a normal law here ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "So, the property of the mean of samples lets us predict : \n",
    "* the probability that a given sample mean will fall within a certain distance of the real mean.\n",
    "* the probability that the real mean will fall within a certain distance of a sample mean.\n",
    "\n",
    "Both can be defined using the size of the interval, centered on the mean, that contains 95% of the probability density. \n",
    "To capture 95% of the density we use the quantiles 0.025 and 0.975 of the PDF. These quantiles are computed using `.ppf` (percent-point function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanOriginal = 1 # the mean of this exponential law is 1\n",
    "stdevOriginal= 1 # its standard deviation is 1\n",
    "\n",
    "sampleSize = 500\n",
    "mu = meanOriginal\n",
    "stdev = stdevOriginal / np.sqrt(sampleSize)\n",
    "\n",
    "\n",
    "# interval computed from the normal law. \n",
    "# 95% of sampled means should fall within that interval. Let's test this\n",
    "CI = stats.norm.ppf([0.025,0.975] , loc = mu , scale = stdev )\n",
    "\n",
    "sampled = np.array( [ sampling(sampleSize).mean() for i in range(10000) ] )\n",
    "\n",
    "nbInside = sum( ( sampled<CI[1] ) * ( sampled>CI[0] ) )\n",
    "print(nbInside , 'out of', 10000 , 'sampled means fall within the 95% confidence interval' , CI)\n",
    "\n",
    "fig=plt.figure(figsize=(14,5))\n",
    "sns.histplot(x=sampled , binwidth=0.005, multiple=\"stack\",\n",
    "             hue = [ ( x<CI[1] ) and ( x>CI[0] )  for x in sampled ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's see how many times the real mean falls within the confidence interval centered on the sample mean.\n",
    "nbInside =0\n",
    "for i in range(10000):\n",
    "    sampledMean = sampling(sampleSize).mean()\n",
    "    CI = stats.norm.ppf([0.025,0.975] , loc = sampledMean , scale = stdev )\n",
    "    \n",
    "    if meanOriginal<CI[1] and meanOriginal>CI[0]:\n",
    "        nbInside+=1\n",
    "\n",
    "print(nbInside , 'out of', 10000 , \n",
    "      'times, the real mean falls within the 95% confidence interval of the sampled mean.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here, we were able to generate a confidence interval around the sampled mean such that 95% of the time\n",
    "such an interval contains the real mean: this is the confidence level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**overview of functions to manipulate the theoretical distributions**\n",
    "\n",
    "* **.pdf / pmf** : takes a value and return the density / probability of drawing this particular value\n",
    "* **.cdf** : takes a value and return the probability of drawing this value or less\n",
    "* **.ppf** : takes a quantile and returns the corresponding value (inverse of cmf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise 02  <a id='2'></a>\n",
    "\n",
    "\n",
    "Say you toss 10 coins, and obtain heads 7 times.\n",
    "\n",
    "This experiment is equivalent to a binomial law with n=10 and p=0.5. `stats.binom(n=10,p=0.5)`\n",
    "Using `.pmf()` , `.cdf()`, and/or `.ppf()`, answer the following questions :\n",
    "\n",
    "1. How likely was this result, provided the coin is fair? \n",
    "2. How likely was it to come up with at most 7 heads , provided the coin is fair? \n",
    "3. How likely was it to come up with at least 7 heads , provided the coin is fair? \n",
    "4. How likely was it to come up with a result at least as different from the expected mean of 5, provided the coin is fair? \n",
    "5. How about if you come up with 1 heads out of 10 ? Do you think the coin is fair in that case? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How likely was this result, provided the coin is fair? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-12 solutions/solution_02_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How likely was it to come up with at most 7 heads , provided the coin is fair? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 13-21 solutions/solution_02_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How likely was it to come up with at least 7 heads , provided the coin is fair? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 22-26 solutions/solution_02_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How likely was it to come up with a result at least as different from the expected mean of 5, provided the coin is fair? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 27-32 solutions/solution_02_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How about if you come up with 1 heads out of 10 ? Do you think the coin is fair in that case? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 33- solutions/solution_02_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# 2. Statistical hypothesis testing  <a id='3'></a>\n",
    "\n",
    "Recipe for a statistical hypothesis test :\n",
    "\n",
    "1. Define a **null hypothesis** and an **alternative hypothesis**. These hypotheses are statements about the “real world”\n",
    "2. Define a **test statistic** (e.g. the mean of a sample) and its expected behaviour under the null hypothesis.\n",
    "3. Collect data and calculate the **observed value of the test statistic**.\n",
    "4. Compare the observed test statistic to its expected values under the null hypothesis\n",
    "\n",
    "\n",
    "A **test statistic** is a numerical summary derived from a sample, which must be calculable (exactly or approximately) from its sampling distribution under the null hypothesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example \n",
    "\n",
    "Say we sample $n$ measures from a normal law of unknown mean $m$ and known variance $\\sigma$, which we note $N(m,\\sigma)$.\n",
    "\n",
    "* null hypothesis : $H_0 = m = m_0$ (for instance $m_0$ can be a reference value coming from another population)\n",
    "* alternative hypothesis : $H_1 = m \\neq m_0$\n",
    "* According to the CLT (see above) the mean of a sample of $n$ measures here should follow a normal distribution : $\\bar{x} \\sim N(m , \\sigma / \\sqrt{n} )$. \n",
    "* **Under the null hypothesis** : $\\bar{x} \\sim N(m_0 , \\sigma / \\sqrt{n} )$\n",
    "\n",
    "In practice it is more practical to **center and scale** this distribution:\n",
    "\n",
    "$$\\bar{x} \\sim N(m_0 , \\sigma / \\sqrt{n} ) \\rightarrow \\frac{\\bar{x} - m_0}{\\sigma / \\sqrt{n}} \\sim N( 0 , 1 )$$\n",
    "\n",
    "* **test statistic** : \n",
    "$$\\frac{\\bar{x} - m_0}{\\sigma / \\sqrt{n}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's have $m_0 = 1$, $\\sigma = 2$, and say we collected $n=100$ measures and their mean is $\\bar{x}=1.42$.\n",
    "\n",
    "Thus the test statistic is $\\frac{\\bar{x} - m_0}{\\sigma / \\sqrt{n}} = 2.1$\n",
    "\n",
    "What is the probability of obtaining a test statistic at least as extreme as the observed one under the null hypothesis ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.linspace(-5,5,100)\n",
    "fig,ax=plt.subplots(figsize=(14,7))\n",
    "sns.lineplot(x=x , y=stats.norm.pdf(x) )\n",
    "x1= np.linspace(-5,-2.1,100)\n",
    "plt.fill_between(x1, stats.norm.pdf(x1) , color='red')\n",
    "x2= np.linspace(2.1,5,100)\n",
    "plt.fill_between(x2, stats.norm.pdf(x2) , color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testStatistic = 2.1\n",
    "probaMoreExtreme = ( stats.norm.cdf(- abs(testStatistic) )  +   # probability of observing <= -2.1\n",
    "                   ( 1-stats.norm.cdf( abs(testStatistic) ) ) ) # probability of observing >=  2.1\n",
    "print(probaMoreExtreme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the definition of the **p-value** : the probability of obtaining a test statistic at least as extreme as the observed one under the null hypothesis.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### two-sided or one-sided test?\n",
    "\n",
    "Above, you can see that when we get the p-value we account for test-statistics at least as extreme both in the negative and in the positive.\n",
    "\n",
    "This is what we call a **two-sided** hypothesis test, where $H_1 = m \\neq m_0$ includes the possibility that $m>m_0$ as well as $m<m_0$.\n",
    "\n",
    "It is possible to perform a **one-sided** hypothesis test, where for example, $H_1 = m > m_0$, in which case the p-value would account for the sign of the test statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testStatistic = 2.1\n",
    "probaMoreExtreme = ( 1-stats.norm.cdf( abs(testStatistic) ) )  # probability of observing >= 2.1\n",
    "print(\"one-sided p-value :\",probaMoreExtreme)\n",
    "\n",
    "x= np.linspace(-5,5,100)\n",
    "fig,ax=plt.subplots(figsize=(14,7))\n",
    "sns.lineplot(x=x , y=stats.norm.pdf(x) )\n",
    "x2= np.linspace(testStatistic,5,100)\n",
    "plt.fill_between(x2, stats.norm.pdf(x2) , color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the distribution of the test statistic is symetric, so the 2-sided p-value is twice that of the 1-sided p-value (if it is <0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.1 t-test : difference between two means  <a id='4'></a>\n",
    "\n",
    "The example test we have just shown requires the population standard deviation to be known in advance, which is, in practice, often not the case.\n",
    "\n",
    "When the population variance is unknown, one can **estimate the variance using the sample**, but then the test statistic does not follow a normal distribution but rather a **t-distribution with $n-1$ degrees of freedom.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.linspace(-5,5,100)\n",
    "fig,ax=plt.subplots(figsize=(14,7))\n",
    "sns.lineplot(x=x , y=stats.norm.pdf(x) , label='normal distribution N(0,1)')\n",
    "sns.lineplot(x=x , y=stats.t.pdf(x,df=3) , label='t-distribution with\\n3 degrees of freedom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This t-distribution can also be used to test the (in)equality of the mean of two unpaired samples.\n",
    "\n",
    "Consider two populations, of respective means $\\mu_1$ and $\\mu_2$, from which we sample $n_1$ and $n_2$ individuals and observe the sample means $\\bar{x}_1$ and $\\bar{x}_2$ and standard deviations $s_1$ and $s_2$.\n",
    "\n",
    "The null hypothesis for this t-test is $H_0 = \\mu_1 = \\mu_2$, while the alternative hypothesis is $H_1 = \\mu_1 \\neq \\mu_2$.\n",
    "\n",
    "There is a number of assumptions around that test:\n",
    " * the mean of each sample are normally distributed. When the sample size is large enough, the CLT ensures that part\n",
    " * the data used to carry out the test should be sampled independently from the two populations being compared\n",
    "\n",
    "The t-test of independance, also called Welch's t-test:\n",
    "\n",
    "- our test statistic: \n",
    "$$\\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{(s_1^2/n_1+s_2^2/n_2)}}$$\n",
    "\n",
    "- with the variances calculated as:\n",
    "$$s_1^2 = \\frac{\\sum_{i=1}^{n_1}(x_i-\\bar{x_1})^2}{n_1-1}$$\n",
    "$$s_2^2 = \\frac{\\sum_{j=1}^{n_2}(x_j-\\bar{x_2})^2}{n_2-1}$$\n",
    "\n",
    "And the test statistic under the null hypothesis shall follow a t-distribution with degrees of freedom equal to :\n",
    "\n",
    "$$df = \\frac{[\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n2}]^2}{\\frac{(\\frac{s_1^2}{n_1})^2}{n_1-1} + \\frac{(\\frac{s_2^2}{n_2})^2}{n_2-1}}$$\n",
    "\n",
    "> note : if the two samples variance are equal there is another way of computing the degree of freedom that gives a bit more statistical power. But this requires that you test for variance equality first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_data = pd.read_csv( 'data/mice_data.csv' ) # data about the weight of mices of different genotypes and subjected to different diets\n",
    "sns.catplot(x='weight' , y='diet' , data=mice_data , kind='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOWdata = mice_data['weight'][ mice_data['diet'] == 'CHOW' ]\n",
    "HFDdata = mice_data['weight'][ mice_data['diet'] == 'HFD' ]\n",
    "\n",
    "n1 = len(CHOWdata)\n",
    "n2 = len(HFDdata)\n",
    "\n",
    "CHOWmean = np.mean(CHOWdata)\n",
    "HFDmean = np.mean(HFDdata)\n",
    "\n",
    "CHOWsigmaSq = np.var(CHOWdata , ddof=1) # ddof, for delta degree-of-freedom handles the n-1 dividor\n",
    "HFDsigmaSq = np.var(HFDdata , ddof=1)\n",
    "\n",
    "tstat = (CHOWmean - HFDmean) / (np.sqrt((CHOWsigmaSq/n1) + (HFDsigmaSq/n2)))\n",
    "\n",
    "print('test statistic : ',tstat , sep='\\t')\n",
    "\n",
    "df = (( CHOWsigmaSq / n1 + HFDsigmaSq/n2 )**2) / (((CHOWsigmaSq/n1)**2/(n1-1)) + (( HFDsigmaSq /n2)**2/(n2-1)) )\n",
    "print('degree of freedom : ',df , sep='\\t')\n",
    "pval = stats.t.cdf(-abs(tstat),df=df) + (1-stats.t.cdf(abs(tstat),df=df))\n",
    "print('p-value : ',pval , sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woof, it works, but it is a bit tedious to code.\n",
    "\n",
    "Of course, there exists a function in the `stats` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstat , pval = stats.ttest_ind( CHOWdata , HFDdata , equal_var=False)#use equal_var=True if you have tested for variance equality\n",
    "print('test statistic : ',tstat , sep='\\t')\n",
    "print('p-value : ',pval , sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same results.\n",
    "\n",
    "**micro-exercise**\n",
    "1. Given these results, what is our conclusion there ?\n",
    "2. if the p-value was 0.3, what would be our conclusion ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.2 t-test assumptions and what to do when they are not respected  <a id='5'></a>\n",
    "\n",
    "As you have seen, a p-value is valid with respect to a particular null-hypothesis that relies on a number of assumptions regarding the data behavior.\n",
    "\n",
    "If these **assumptions are false, then the p-value loses its meaning.**\n",
    "\n",
    "Let's get back to the assumptions of Welch's t-test:\n",
    " 1. the data used to carry out the test should be sampled independently from the two populations being compared\n",
    " \n",
    " 2. the mean of each sample are normally distributed. When the sample size is large enough, the CLT ensures that part\n",
    "\n",
    "Point `1.` should be taken care of when designing the experiment and collecting the data. Note that there also exists a t-test when all points are paired (e.g. when you have data for the effect of 2 drugs for each patients). \n",
    "\n",
    "For point `2.`, ideally the samples themselves should follow a normal distribution (this is the original requirement of Student's t-test). Otherwise you should make sure the sample size is large enough for the sample mean to follow something resembling a normal law.\n",
    "\n",
    "Note that there exists no set sample size above which you are in the clear : it is dependent on the particulars of the distribution(s) you sample from.\n",
    "\n",
    "Let's demonstrate how things can go wrong.\n",
    "Take two samples from the same distribution and perform a t-test on them ; if the t-test is appropriate then \n",
    "we should see a p-value < 0.05 in about 5% of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = n2 = 50\n",
    "\n",
    "#experimental setup : we sample twice from the same distribution normal distribution\n",
    "mu, sigma = 5, 3\n",
    "nbUnder05 = 0\n",
    "PV_norm = []\n",
    "for i in range(10000):\n",
    "    sample1 = np.random.normal( mu, sigma , size = n1 )\n",
    "    sample2 = np.random.normal( mu, sigma , size = n2 )\n",
    "    tstat,pval = stats.ttest_ind(sample1,sample2 , equal_var=True)\n",
    "    PV_norm.append(pval)\n",
    "    if pval < 0.05:\n",
    "        nbUnder05 += 1\n",
    "print('normal law','proportion of sampled p-values under 0.05',nbUnder05/10000,sep='\\t')\n",
    "\n",
    "\n",
    "#second setup : rather than sampling in a normal law, we will sample here in a pareto distribution\n",
    "#                 https://en.wikipedia.org/wiki/Pareto_distribution\n",
    "nbUnder05 = 0\n",
    "PV_bin = []\n",
    "for i in range(10000):\n",
    "    sample1 = np.random.pareto( a=1 , size = n1 ) \n",
    "    sample2 = np.random.pareto( a=1 , size = n2 ) \n",
    "    tstat,pval = stats.ttest_ind(sample1,sample2 , equal_var=True)\n",
    "    PV_bin.append(pval)\n",
    "    if pval < 0.05:\n",
    "        nbUnder05 += 1\n",
    "print('pareto law','proportion of sampled p-values under 0.05',nbUnder05/10000,sep='\\t')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1 , figsize=(10,7))\n",
    "sns.histplot( PV_bin  , label = \"pareto\" , binwidth=0.02 , element=\"step\", color = 'xkcd:salmon', ax = ax )\n",
    "sns.histplot( PV_norm , label = \"normal\" , binwidth=0.02 , element=\"step\", color = 'xkcd:mint', ax = ax )\n",
    "ax.legend()\n",
    "ax.set_title(\"P-values of t-test of 2 samples drawn from the same distribution\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the p-value is only as good as long as the assumptions of the null hypothesis hold.\n",
    "\n",
    "> Simulating the behavior of data under the null hypothesis is an excellent way to explore how these tests work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to check that the assumption of normality for the mean of the sample is to check the normality of the data itself\n",
    "\n",
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.2.1 testing normality  <a id='6'></a>\n",
    "\n",
    "There exists several methods to test for normality and we recommend to combine a graphical method \n",
    "with a classic statistical test in order to get the best view of one's data.\n",
    "\n",
    "You can get a graphical overview of the goodness-of-fit of a sample with a Quantile-Quantile-plot, or **QQplot**. \n",
    "The idea here is to compute a number of quantiles on your sample's values and plot them with the expected quantiles for the normal law.\n",
    "**Samples drawn from a normal law will form a diagonal line.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleN = np.random.normal(size = 100) #sample from a normal law\n",
    "sampleE = np.random.exponential(size = 100) # sampling from another distribution\n",
    "\n",
    "fig,axes = plt.subplots(1,2 , figsize = (15,7) )\n",
    "_ = stats.probplot( sampleN , plot=axes[0] )\n",
    "_ = stats.probplot( sampleE , plot=axes[1] )\n",
    "# note : \n",
    "# * stats.probplot plots against the normal law by default, but that can be changed\n",
    "# * the function returns theoretical and observed quantile, which are not interesting to us here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the statistical test, we propose the **Shapiro-Wilk** test. \n",
    "It has been shown to have a slighly better power than other tests by a [2011 study](https://web.archive.org/web/20150630110326/http://instatmy.org.my/downloads/e-jurnal%202/3.pdf).\n",
    "\n",
    "This test relies on the comparison between each sorted point in the observed distribution and its expected value in a normal distribution. \n",
    "Originally proposed for no more than 50 points, refinements to the algorithm now makes this test reliable up to 5000 points (as mentionned in `help(scipy.stats.shapiro)`) ; above this number another test, such as D'agostino's  (`scipy.stats.normaltest`), should be used instead.\n",
    "\n",
    "This test's null hypothesis is normality, so a small p-value corresponds to a rejection of normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstat,pval = stats.shapiro(sampleN)\n",
    "print('normal data     ',\"Shapiro-Wilk's normality test p-value\",pval , sep='\\t')\n",
    "tstat,pval = stats.shapiro(sampleE)\n",
    "print('exponential data',\"Shapiro-Wilk's normality test p-value\",pval, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**micro-exercise**\n",
    "\n",
    "Check the normality for the weights of mices subjected to the CHOW and HFD diets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOWdata = mice_data['weight'][ mice_data['diet'] == 'CHOW' ]\n",
    "HFDdata = mice_data['weight'][ mice_data['diet'] == 'HFD' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data itself is not normal,\n",
    "a good knowledge of the measure you are taking and a large enough sample size may allow you to presume that the mean of the samples are normally distributed.\n",
    "\n",
    "However in the general case it is then better to avoid using the t-test and prefer another type of test that do not make the assumption of normality of the mean : **nonparametric tests**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.2.2 nonparametric testing : Mann–Whitney U test  <a id='7'></a>\n",
    "\n",
    "**Nonparametric statistics** is the branch of statistic that does not rely on the known families of distributions (normal, binomial, exponential, ...) that requires specified parameters (mean and variance, $n$ and $p$ , $\\lambda$ , ... ). \n",
    "Thus, nonparametric statistics is particularly useful when assumptions of normality do not hold.\n",
    "\n",
    "The **Mann–Whitney U test** is a nonparametric test \n",
    "to determine if two samples have different locations\n",
    "\n",
    "\n",
    "**Assumptions**\n",
    " * All observations from samples are independent of each other\n",
    " * The values in each samples are ordinal (i.e.,thay can be compared)\n",
    "\n",
    "**H0** : the probability that a randomly selected value from the first sample is lower than a randomly selected value from the second sample is equal to the probability of being greater.\n",
    "\n",
    "**Test statistic** : $U$, which is computed by looking at each observation $i_1$ in the first sample and counting $1$ for each observation lower than $i_1$ in the second sample (count 0.5 for ties).\n",
    "\n",
    "![Mann-Whitney U illustration](images/MWU.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Say you have two samples:\n",
    "* sample1 : 1.2 1.5 2.2 3.0 3.1 \n",
    "* sample2 : 1.1 1.8 2.2 2.4 2.8 \n",
    "\n",
    "for each observation in sample1, let's count the number of values lower to it in sample2 :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1= [ 1.2 , 1.5 , 2.3 , 3.0 , 3.1 ]\n",
    "sample2= [ 1.1 , 1.8 , 2.2 , 2.4 , 2.8 ]\n",
    "def computeU(sampleA , sampleB):\n",
    "    U1=0\n",
    "    U2=0\n",
    "    for i in sampleA:\n",
    "        for j in sampleB:\n",
    "            if j<i:\n",
    "                U1 += 1\n",
    "            elif i==j:\n",
    "                U1 += 0.5\n",
    "                U2 += 0.5\n",
    "            else:\n",
    "                U2 += 1\n",
    "    return min(U1,U2)\n",
    "U_manual = computeU(sample1 , sample2)\n",
    "\n",
    "U1_scipy , pval = stats.mannwhitneyu(sample1,sample2,method = 'exact' , alternative='two-sided') \n",
    "U2_scipy , pval = stats.mannwhitneyu(sample2,sample1,method = 'exact' , alternative='two-sided') \n",
    "U_scipy = min(U1_scipy , U2_scipy) \n",
    "# scipy implements a computation of U that only accounts for cases where sample1 is above sample2 ...\n",
    "\n",
    "print('U manual :',U_manual)\n",
    "print('U scipy  :',U_scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**important note :** for versions of scipy < 1.7 , the p-value of `scipy.stats.mannwhitneyu` is only valid when the number of observations in both samples is above 20 (see [their doc](https://docs.scipy.org/doc/scipy-1.6.0/reference/generated/scipy.stats.mannwhitneyu.html)). If the number of observations is < 20, you have to rely on old-fashioned tables such as [this one from uMBoston](http://ocw.umb.edu/psychology/psych-270/other-materials/RelativeResourceManager.pdf).\n",
    "> To use this table, look up the number in the cell corresponding to the size of both samples and the desired confidence level. If U is under this number then the test is significant and H0 is rejected.\n",
    "Here, $n1=n2=5$, for a confidence level of $0.05$ then the test is significant only when $U<2$.\n",
    "\n",
    "\n",
    "Note that **the null hypothesis of the Mann-Whitney U test is different from the one of the t-test**.\n",
    "The t-test looks for equality of the means, while the U tests for equiprobabilities of seing lower/higher values.\n",
    "\n",
    "This means that they will sometimes behave quite differently (See for instance [Fagerland2012](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-12-78), which shows that Mann-Whitney U rejects $H_0$ with a high frequency when two samples only differ in variance and not in mean or median).\n",
    "\n",
    "It is important to keep this in mind when interpretating results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The U statistics is aequivalent to $R_1 - \\frac{n_1 (n_1 - 1 )}{2}$ , where $R_1$ is the sum of ranks of points in one of the sample. This is why this test is sometimes refered to as a **rank-sum test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**micro-exercise**\n",
    "\n",
    "Perform the Mann-Withney U test on the mice data. \n",
    "What is your conclusion?\n",
    "\n",
    "> here, there is more than 20 observation in both samples, so you can rely on the p-value returned by `scipy.stats.mannwhitneyu` even if there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.3 p-value, power and errors  <a id='8'></a>\n",
    "\n",
    "Now that you have seen a couple of statistical tests, \n",
    "let's explore in more details how to evaluate a test results and the uncertainties around it.\n",
    "\n",
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.3.1 errors of type I and II  <a id='9'></a>\n",
    "\n",
    "* **type I  error** : rejecting $H_0$ while it is True  (e.g., detecting an illness in an uninfected patient)\n",
    "* **type II error** : accepting $H_0$ while it is False (e.g., not detecting an illness in an infected patient)\n",
    "\n",
    "![Type_I_and_II_error](images/Type_I_and_II_error.jpg)\n",
    "Image source: unbiasedresearch.blogspot.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inferiential_table](images/error_type_table.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of committing type I error, commonly noted $\\alpha$, is usually controled and set to 0.05, or 0.01.\n",
    "\n",
    "The probability of committing type II error, commonly noted $\\beta$, is more complex to control and can only be set for specific effect sizes. In practice we rather refer to $1-\\beta$, called the **power** of the test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.3.2 P-values  <a id='10'></a>\n",
    "\n",
    "* The **p-value** is the probability of obtaining a test statistic **as or more extreme** as the observed one, **if the null hypothesis is true**\n",
    "* In other words, how likely would it be to observe such a large test statistic \"by chance\", if there is no true underlying signal?\n",
    "* A given p-value is **always** linked to a specific null hypothesis. \n",
    "\n",
    "There are many misconceptions about p-values (see e.g. [Goodman (2008)](http://www.sciencedirect.com/science/article/pii/S0037196308000620))\n",
    "* A p-value is **not** the probability that the null hypothesis is correct\n",
    "* A p-value is **not** the probability that we are making an error\n",
    "* A large p-value does **not** prove that there is no true signal\n",
    "* Always report the exact p-value - given today's computers there is no reason to just say $p<0.05$\n",
    "\n",
    "In the presence of a true effect, p-values are affected by the sample size, since our estimates are getting more precise with larger sample sizes\n",
    "\n",
    "\n",
    "**Example :** two samples of $n$ observations each, mean difference = 0 or 1. Let's look at the t-tests p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTtest( n=100 , d=0 , std = 1): \n",
    "    # draws 2 samples with a set difference in mean and return the t statistic and p-value of a t-test\n",
    "    t , pval_ttest = stats.ttest_ind( np.random.randn( n ) * std , np.random.randn( n ) * std + d ,equal_var=True)\n",
    "    return t,pval_ttest\n",
    "    \n",
    "\n",
    "D , N , PVAL = [],[] , []\n",
    "d=1\n",
    "for d in [0,1]:\n",
    "    for n in [3,10,100]: # 3 sizes of samples\n",
    "        for x in range(1000): # repeat a 1000 time to get an idea of variation\n",
    "            tstat, pval = testTtest(n=n , d=d)\n",
    "            PVAL.append( pval )\n",
    "            N.append(n)\n",
    "            D.append(d)\n",
    "\n",
    "# we push to a dataframe for easy plotting\n",
    "df= pd.DataFrame( { 'n':N , 'pvalue': PVAL , 'difference' : D} )\n",
    "sns.catplot( x='difference' , y='pvalue' , hue='n', data=df  , kind='box' , height=8 , aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.3.3 power  <a id='11'></a>\n",
    "\n",
    "The power of any statistical test is the **probability of rejecting a false negative hypothesis**. \n",
    "It is defined as $1−\\beta$ where $\\beta$ is the probability of type II error.\n",
    "\n",
    "Power depends on :\n",
    "* a specific null hypothesis\n",
    "* a specific experimental setup (sample sizes)\n",
    "* a specific state of reality (e.g., the real difference between population means)\n",
    "\n",
    "In practice the \"specific state of reality\" often corresponds to a given **effect size**. \n",
    "When comparing means - as in the t-test - the effect size correspond to the (absolute) difference between the two means divided by the standard deviation.\n",
    "\n",
    "**Example** : \n",
    "We use a t-test to detect eventual difference in mean between two populations using two samples of size 20.\n",
    "\n",
    "If the difference between populations is $2$, and they have the same standard deviation of $3$,\n",
    "what is the probability that we will detect a difference at a significance level of 0.05 (i.e., that we will reject $H_0$ when the p-value<0.05) ?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_difference = 2\n",
    "standard_dev = 3\n",
    "sample_size = 20\n",
    "sig_level = 0.05\n",
    "\n",
    "## simulation \n",
    "N=10000\n",
    "nbRejected = 0\n",
    "tstats = []\n",
    "rejected=[]\n",
    "for i in range(N):\n",
    "    # I use the testTtest function previously defined to simulate the test\n",
    "    t,pval = testTtest( n=sample_size , d=mean_difference , std=standard_dev ) \n",
    "    tstats.append(t)\n",
    "    rejected.append( pval<sig_level )\n",
    "\n",
    "df= pd.DataFrame( {  'rejected': rejected , 't' : tstats} )\n",
    "\n",
    "\n",
    "# For this test, the level of significance is\n",
    "threshold = stats.t.ppf( 1-sig_level/2  , df = 2*sample_size - 2 )\n",
    "\n",
    "## plotting our results\n",
    "x= np.linspace(-5,5,100)\n",
    "fig,ax=plt.subplots(figsize=(14,7))\n",
    "sns.lineplot(x=x , y=stats.t.pdf(x , df = 2*sample_size - 2 )  , label = 'null hypothesis')\n",
    "xAccepted = np.linspace(-threshold , threshold , 50 )\n",
    "plt.fill_between(xAccepted, stats.norm.pdf(xAccepted) , color='red')\n",
    "\n",
    "sns.histplot( x=df.loc[ df.rejected==True ,'t']  , \n",
    "             binwidth=0.04 , element='step' , color='xkcd:teal',\n",
    "             stat='density' , label = 'correctly rejected')\n",
    "sns.histplot( x=df.loc[ df.rejected==False ,'t'] , \n",
    "             binwidth=0.04 , element='step' , color='xkcd:salmon', \n",
    "             stat='density' , label = 'spuriously accepted')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "print('test theshold:',threshold)\n",
    "print('Fraction of rejected (~power):', sum(df.rejected)/N )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we often have to rely on simulation procedures (as shown above) to estimate statistical power. \n",
    "However for some classical statistical tests like the t-test there exists libraries to compute statistical power: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "effect_size = mean_difference/standard_dev\n",
    "\n",
    "P = TTestIndPower()\n",
    "print( 'power:' , P.power(effect_size=effect_size , nobs1=sample_size , ratio=1 , alpha=0.05) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> note the argument `ratio` , which is the number of observations in sample 2 relative to\n",
    "        sample 1. in other words : `ratio = n2/n1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating statistical power can help inform our experimental design**. \n",
    "\n",
    "For example, how many observation per sample do we need if we want to detect a difference in mean of 1 with significance level (type I error) 0.01 and statistical power 0.80:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size=1\n",
    "sig_threshold = 0.01\n",
    "P = TTestIndPower()\n",
    "\n",
    "powers = []\n",
    "for sample_size in range(2,50):\n",
    "    powers.append( P.power(effect_size=effect_size , nobs1=sample_size , ratio=1 , alpha=sig_threshold) )\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(14,7))\n",
    "sns.lineplot(x=range(2,50) , y=powers , label = 'effect size='+str(effect_size) , ax=ax)\n",
    "ax.axhline(0.8, color='r', linestyle='-')\n",
    "ax.set(xlabel='sample size', ylabel='power')\n",
    "\n",
    "\n",
    "## or, directly:\n",
    "print( 'minimum sample size:', P.solve_power(effect_size=effect_size , nobs1=None ,  ratio=1 , alpha=sig_threshold , power = 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise 03  <a id='12'></a>\n",
    "\n",
    "Consider the mice data loaded in the `mice_data` DataFrame above.\n",
    "\n",
    "1. Compute the effect size of the diet on the weights \n",
    "2. Compute the statistical power of the corresponding t-test for that effect size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_data = pd.read_csv( 'data/mice_data.csv' ) # data about the weight of mices of different genotypes and subjected to different diets\n",
    "sns.catplot(x='weight' , y='diet' , data=mice_data , kind='violin')\n",
    "\n",
    "CHOWdata = mice_data['weight'][ mice_data['diet'] == 'CHOW' ]\n",
    "HFDdata = mice_data['weight'][ mice_data['diet'] == 'HFD' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the effect size of the diet on the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-22 solutions/solution_02_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the statistical power of the corresponding t-test for that effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 23- solutions/solution_02_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.4 BONUS : multiple testing   <a id='13'></a>\n",
    "\n",
    "\n",
    "## Multiple hypothesis testing\n",
    "\n",
    "Recall the definition of the p-value: the probability of obtaining a test statistic at least as extreme as the one observed, **if the null hypothesis is true**\n",
    "\n",
    "Thus, *even* if the p-value is, let's say, 0.04, there is still a 4% chance of obtaining such an extreme result by chance.\n",
    "\n",
    "This is often acceptable if we only perform one test, if we perform many tests we have seen (with the simulations), that even when there is no real effect some tests will turn out significant by chance.\n",
    "\n",
    "> This is the definition of the $\\alpha$ risk fo type I error is.\n",
    "\n",
    "Of course, this has important implication for science and the relevance of our results.\n",
    "\n",
    "![xkcd882.png](images/xkcd882.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> source : [xkcd](http://xkcd.com) (note: there many relevant xkcd strips for this course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([1, 2, 5, 10, 50, 100])\n",
    "fig,ax=plt.subplots(figsize=(14,7))\n",
    "sns.lineplot( x=P , y=1- 0.95**P ,ax=ax)\n",
    "ax.set(xlabel='number of tests', ylabel='probability of at least 1 test significant by chance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "We need to change perspective.\n",
    "Instead of trying to limit the false positive probability for *each* test, we focus on:\n",
    "* the probability of obtaining **any** false positives (family-wise error rate, **FWER**)\n",
    "* the proportion of false positives among all findings (false discovery rate, **FDR**)\n",
    "\n",
    "> Controlling the FWER is often too stringent - limit type I errors, but get lots of type II errors. \n",
    "\n",
    "### The Bonferroni method for controlling the FWER\n",
    "\n",
    "- Assume we are performing $N$ tests\n",
    "- To control the FWER at (e.g.) 0.05, only call variables with p-values below $0.05/N$ significant\n",
    "\n",
    "### The Benjamini-Hochberg method for controlling the FDR\n",
    "\n",
    "- Assume we are performing $N$ tests\n",
    "- Intuition: for each p-value threshold $\\alpha$, we can estimate the number of false discoveries to be at most $\\alpha N$\n",
    "- Compare this to the actual number of discoveries at the threshold - $N_\\alpha$\n",
    "- Choose a p-value threshold $\\alpha$ such that $\\alpha N/N_\\alpha$ is less than a desired threshold (e.g. 0.05) - this threshold would give an expected FDR of 0.05\n",
    "- Note that the FDR is truly a property of a *set* - in a set of genes with FDR = 0.05, we can expect around 5% to be false discoveries. However, we don't know *which* ones! It could be the most significant!\n",
    "- Often, we want a gene-wise measure of significance (like the p-value)\n",
    "- The q-value, or adjusted p-value, of a variable is the *smallest* FDR we have to accept in order to call that variable significant.\n",
    "- For example, if the adjusted p-value is 0.2, we have to accept that if we want to call this variable (and consequently, all variables with lower p-values) significant, there will be approximately 20% false discoveries among them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we perform 10 000 tests of 10 000 random data-sets\n",
    "pvals = []\n",
    "\n",
    "N = 10000\n",
    "mean_difference = 0 # no differences -> any detected difference is due to chance\n",
    "sample_size = 100\n",
    "std=1\n",
    "\n",
    "for i in range(N):\n",
    "    t , pval_ttest = stats.ttest_ind( np.random.randn( sample_size ) * std , \n",
    "                                     np.random.randn( sample_size ) * std + mean_difference ,equal_var=True)\n",
    "    pvals.append(pval_ttest)\n",
    "    \n",
    "pvals = np.array(pvals)\n",
    "# stats models proposes a function implementing numerous p-value correction methods\n",
    "# https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "rejected,fwers,alphacSidak,alphacBonf = multipletests(pvals, alpha=0.05, method='bonferroni')\n",
    "rejected,fdrs,alphacSidak,alphacBonf = multipletests(pvals, alpha=0.05, method='fdr_bh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(15, 5))\n",
    "sns.histplot( pvals ,binwidth=0.02 ,  ax = ax[0] , label = 'p-value' , color='xkcd:vomit')\n",
    "sns.histplot( fwers ,binwidth=0.02 ,   ax = ax[0] , label = 'FWER' , color='xkcd:tomato')\n",
    "sns.histplot( fdrs  ,binwidth=0.02 ,  ax = ax[0] , label = 'FDR' , color='xkcd:lavender')\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].scatter( pvals , fwers , label='FWER' , c='xkcd:tomato' , alpha=0.5)\n",
    "ax[1].scatter( pvals , fdrs , label='FDR' , c='xkcd:lavender', alpha=0.5)\n",
    "ax[1].set_xlabel('p-value')\n",
    "ax[1].set_ylabel('corrected value')\n",
    "\n",
    "print('Fraction of (spuriously) significant tests:')\n",
    "print('p-value:' , sum(pvals<0.05)/N )\n",
    "print('FWER   :' , sum(fwers<0.05)/N )\n",
    "print('FDR    :' , sum(fdrs <0.05)/N )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, imagine a different scenario where 100 out of the 10 000 tests are actually different\n",
    "pvals = []\n",
    "\n",
    "N = 10000 - 100\n",
    "mean_difference = 0 # no differences -> any detected difference is due to chance\n",
    "sample_size = 100\n",
    "std=1\n",
    "\n",
    "for i in range(N):\n",
    "    t , pval_ttest = stats.ttest_ind( np.random.randn( sample_size ) * std , \n",
    "                                     np.random.randn( sample_size ) * std + mean_difference ,equal_var=True)\n",
    "    pvals.append(pval_ttest)\n",
    "\n",
    "# now we add the different ones:\n",
    "mean_difference = 0.75\n",
    "for i in range(100):\n",
    "    t , pval_ttest = stats.ttest_ind( np.random.randn( sample_size ) * std , \n",
    "                                     np.random.randn( sample_size ) * std + mean_difference ,equal_var=True)\n",
    "    pvals.append(pval_ttest)\n",
    "\n",
    "    \n",
    "    \n",
    "pvals = np.array(pvals)\n",
    "# stats models proposes a function implementing numerous p-value correction methods\n",
    "# https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "rejected,fwers,alphacSidak,alphacBonf = multipletests(pvals, alpha=0.05, method='bonferroni')\n",
    "rejected,fdrs,alphacSidak,alphacBonf = multipletests(pvals, alpha=0.05, method='fdr_bh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of significant tests:')\n",
    "print('p-value:' , sum(pvals<0.05) )\n",
    "print('FWER   :' , sum(fwers<0.05) )\n",
    "print('FDR    :' , sum(fdrs <0.05) )\n",
    "print()\n",
    "print('Number of correctly significant tests (out of 100):')\n",
    "print('p-value:' , sum(pvals[-100:]<0.05) )\n",
    "print('FWER   :' , sum(fwers[-100:]<0.05) )\n",
    "print('FDR    :' , sum(fdrs[-100:] <0.05) )\n",
    "print()\n",
    "print('Number of spuriously significant tests (out of 9900):')\n",
    "print('p-value:' , sum(pvals[:-100]<0.05) )\n",
    "print('FWER   :' , sum(fwers[:-100]<0.05) )\n",
    "print('FDR    :' , sum(fdrs[:-100] <0.05) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py38)",
   "language": "python",
   "name": "conda_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
