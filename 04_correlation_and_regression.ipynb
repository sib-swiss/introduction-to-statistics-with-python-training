{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content <a id='toc'></a>\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1. correlation](#0)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.1. Pearson's (linear) correlation](#1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.2. Spearman's (rank) correlation coefficient](#2)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.3. Significance of Pearson and Spearman correlation coefficient.](#3)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.4 Kendall tau correlation coefficient (for fun)](#4)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Exercise 01](#5)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.5 Correlation and causation](#6)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.Linear regression](#7)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.1.Presentation](#8)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.Underlying hypothesis](#9)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3. Goodness of fit](#10)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.4. Confidence interval and test statistics](#11)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.5. Maximum Likelihood](#12)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.6. Model choosing](#13)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.7. What to do when some hypothesis about OLS are not true](#14)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Exercise 02](#15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import collections  as mc\n",
    "from operator import itemgetter\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So far we have seen how to evaluate the relationship between\n",
    "* 2 categorical variables (fisher's exact test, chi-square)\n",
    "* 1 quantitative and a categorical variable (t-test, anova)\n",
    "\n",
    "Now we are going to see how to relate 2 quantitative variables together.\n",
    "\n",
    "In this notebook we will use the folloming dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>shoe_size</th>\n",
       "      <th>right_left_handed</th>\n",
       "      <th>smoker_nonsmoker</th>\n",
       "      <th>hair_colour</th>\n",
       "      <th>eye_colour</th>\n",
       "      <th>R_wrist_girth</th>\n",
       "      <th>L_wrist_girth</th>\n",
       "      <th>nb_siblings</th>\n",
       "      <th>birth_place</th>\n",
       "      <th>height_M</th>\n",
       "      <th>height_F</th>\n",
       "      <th>nb_siblings_F</th>\n",
       "      <th>nb_siblings_M</th>\n",
       "      <th>diet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>183</td>\n",
       "      <td>72</td>\n",
       "      <td>42.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>lb</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>2</td>\n",
       "      <td>vd</td>\n",
       "      <td>170</td>\n",
       "      <td>174</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>183</td>\n",
       "      <td>68</td>\n",
       "      <td>43.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>170</td>\n",
       "      <td>178</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>182</td>\n",
       "      <td>73</td>\n",
       "      <td>44.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>bl</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>ne</td>\n",
       "      <td>168</td>\n",
       "      <td>181</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>175</td>\n",
       "      <td>66</td>\n",
       "      <td>41.0</td>\n",
       "      <td>L</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2</td>\n",
       "      <td>vd</td>\n",
       "      <td>156</td>\n",
       "      <td>178</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>158</td>\n",
       "      <td>42</td>\n",
       "      <td>41.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2</td>\n",
       "      <td>vd</td>\n",
       "      <td>156</td>\n",
       "      <td>178</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>179</td>\n",
       "      <td>70</td>\n",
       "      <td>47.0</td>\n",
       "      <td>R</td>\n",
       "      <td>S</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>150</td>\n",
       "      <td>190</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>F</td>\n",
       "      <td>172</td>\n",
       "      <td>61</td>\n",
       "      <td>38.0</td>\n",
       "      <td>L</td>\n",
       "      <td>NS</td>\n",
       "      <td>lb</td>\n",
       "      <td>1</td>\n",
       "      <td>15.3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2</td>\n",
       "      <td>vd</td>\n",
       "      <td>168</td>\n",
       "      <td>186</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>M</td>\n",
       "      <td>185</td>\n",
       "      <td>69</td>\n",
       "      <td>44.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>16.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>172</td>\n",
       "      <td>167</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>M</td>\n",
       "      <td>177</td>\n",
       "      <td>75</td>\n",
       "      <td>43.0</td>\n",
       "      <td>R</td>\n",
       "      <td>S</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>161</td>\n",
       "      <td>169</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>F</td>\n",
       "      <td>165</td>\n",
       "      <td>50</td>\n",
       "      <td>37.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3</td>\n",
       "      <td>vd</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>M</td>\n",
       "      <td>186</td>\n",
       "      <td>93</td>\n",
       "      <td>45.0</td>\n",
       "      <td>R</td>\n",
       "      <td>S</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>18.6</td>\n",
       "      <td>18.7</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>176</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>F</td>\n",
       "      <td>165</td>\n",
       "      <td>53</td>\n",
       "      <td>39.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>lb</td>\n",
       "      <td>1</td>\n",
       "      <td>15.3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ju</td>\n",
       "      <td>166</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>F</td>\n",
       "      <td>156</td>\n",
       "      <td>48</td>\n",
       "      <td>37.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>2</td>\n",
       "      <td>vd</td>\n",
       "      <td>168</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>M</td>\n",
       "      <td>178</td>\n",
       "      <td>75</td>\n",
       "      <td>43.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.8</td>\n",
       "      <td>0</td>\n",
       "      <td>vd</td>\n",
       "      <td>175</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>F</td>\n",
       "      <td>172</td>\n",
       "      <td>49</td>\n",
       "      <td>39.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>lb</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>168</td>\n",
       "      <td>180</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>F</td>\n",
       "      <td>168</td>\n",
       "      <td>52</td>\n",
       "      <td>38.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>lb</td>\n",
       "      <td>2</td>\n",
       "      <td>14.1</td>\n",
       "      <td>13.7</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>165</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>183</td>\n",
       "      <td>72</td>\n",
       "      <td>42.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>lb</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>2</td>\n",
       "      <td>vd</td>\n",
       "      <td>170</td>\n",
       "      <td>174</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>M</td>\n",
       "      <td>178</td>\n",
       "      <td>73</td>\n",
       "      <td>42.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>bresil</td>\n",
       "      <td>165</td>\n",
       "      <td>168</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>M</td>\n",
       "      <td>183</td>\n",
       "      <td>60</td>\n",
       "      <td>43.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>174</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>F</td>\n",
       "      <td>166</td>\n",
       "      <td>48</td>\n",
       "      <td>38.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>lb</td>\n",
       "      <td>4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2</td>\n",
       "      <td>vd</td>\n",
       "      <td>158</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>M</td>\n",
       "      <td>177</td>\n",
       "      <td>70</td>\n",
       "      <td>45.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>bl</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>173</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>M</td>\n",
       "      <td>180</td>\n",
       "      <td>70</td>\n",
       "      <td>43.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4</td>\n",
       "      <td>vd</td>\n",
       "      <td>160</td>\n",
       "      <td>170</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>174</td>\n",
       "      <td>69</td>\n",
       "      <td>42.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>16.7</td>\n",
       "      <td>16.4</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>164</td>\n",
       "      <td>175</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>184</td>\n",
       "      <td>70</td>\n",
       "      <td>46.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>2</td>\n",
       "      <td>vd</td>\n",
       "      <td>160</td>\n",
       "      <td>175</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>168</td>\n",
       "      <td>69</td>\n",
       "      <td>43.0</td>\n",
       "      <td>R</td>\n",
       "      <td>S</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>16.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>153</td>\n",
       "      <td>176</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>F</td>\n",
       "      <td>176</td>\n",
       "      <td>59</td>\n",
       "      <td>39.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>bl</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>166</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>F</td>\n",
       "      <td>159</td>\n",
       "      <td>51</td>\n",
       "      <td>38.0</td>\n",
       "      <td>R</td>\n",
       "      <td>S</td>\n",
       "      <td>db</td>\n",
       "      <td>3</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15.8</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>168</td>\n",
       "      <td>167</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>169</td>\n",
       "      <td>60</td>\n",
       "      <td>41.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>bl</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>vs</td>\n",
       "      <td>157</td>\n",
       "      <td>168</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>F</td>\n",
       "      <td>164</td>\n",
       "      <td>61</td>\n",
       "      <td>38.5</td>\n",
       "      <td>R</td>\n",
       "      <td>S</td>\n",
       "      <td>bl</td>\n",
       "      <td>2</td>\n",
       "      <td>15.5</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1</td>\n",
       "      <td>france</td>\n",
       "      <td>170</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>150</td>\n",
       "      <td>49</td>\n",
       "      <td>35.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>14.5</td>\n",
       "      <td>4</td>\n",
       "      <td>colombie</td>\n",
       "      <td>148</td>\n",
       "      <td>165</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>F</td>\n",
       "      <td>163</td>\n",
       "      <td>54</td>\n",
       "      <td>37.5</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>15.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>3</td>\n",
       "      <td>ju</td>\n",
       "      <td>167</td>\n",
       "      <td>164</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>M</td>\n",
       "      <td>181</td>\n",
       "      <td>96</td>\n",
       "      <td>44.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>lb</td>\n",
       "      <td>1</td>\n",
       "      <td>18.6</td>\n",
       "      <td>19.3</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>164</td>\n",
       "      <td>174</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>169</td>\n",
       "      <td>68</td>\n",
       "      <td>40.0</td>\n",
       "      <td>L</td>\n",
       "      <td>NS</td>\n",
       "      <td>bl</td>\n",
       "      <td>2</td>\n",
       "      <td>14.3</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1</td>\n",
       "      <td>ti</td>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>M</td>\n",
       "      <td>170</td>\n",
       "      <td>65</td>\n",
       "      <td>44.0</td>\n",
       "      <td>R</td>\n",
       "      <td>S</td>\n",
       "      <td>bl</td>\n",
       "      <td>2</td>\n",
       "      <td>15.2</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>F</td>\n",
       "      <td>160</td>\n",
       "      <td>50</td>\n",
       "      <td>34.5</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0</td>\n",
       "      <td>vd</td>\n",
       "      <td>162</td>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>M</td>\n",
       "      <td>180</td>\n",
       "      <td>63</td>\n",
       "      <td>42.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>bl</td>\n",
       "      <td>2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>14.2</td>\n",
       "      <td>2</td>\n",
       "      <td>vs</td>\n",
       "      <td>160</td>\n",
       "      <td>182</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>F</td>\n",
       "      <td>165</td>\n",
       "      <td>49</td>\n",
       "      <td>38.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1</td>\n",
       "      <td>ti</td>\n",
       "      <td>168</td>\n",
       "      <td>182</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>184</td>\n",
       "      <td>80</td>\n",
       "      <td>45.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>ti</td>\n",
       "      <td>162</td>\n",
       "      <td>175</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>F</td>\n",
       "      <td>170</td>\n",
       "      <td>65</td>\n",
       "      <td>40.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2</td>\n",
       "      <td>vs</td>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>F</td>\n",
       "      <td>173</td>\n",
       "      <td>66</td>\n",
       "      <td>40.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>lb</td>\n",
       "      <td>2</td>\n",
       "      <td>16.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ge</td>\n",
       "      <td>175</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>165</td>\n",
       "      <td>63</td>\n",
       "      <td>39.0</td>\n",
       "      <td>L</td>\n",
       "      <td>NS</td>\n",
       "      <td>lb</td>\n",
       "      <td>3</td>\n",
       "      <td>17.2</td>\n",
       "      <td>17.2</td>\n",
       "      <td>0</td>\n",
       "      <td>zh</td>\n",
       "      <td>162</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>159</td>\n",
       "      <td>52</td>\n",
       "      <td>37.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>bl</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>157</td>\n",
       "      <td>170</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>M</td>\n",
       "      <td>181</td>\n",
       "      <td>65</td>\n",
       "      <td>44.0</td>\n",
       "      <td>L</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>17.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>162</td>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>F</td>\n",
       "      <td>175</td>\n",
       "      <td>76</td>\n",
       "      <td>42.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>db</td>\n",
       "      <td>2</td>\n",
       "      <td>16.6</td>\n",
       "      <td>16.8</td>\n",
       "      <td>1</td>\n",
       "      <td>vd</td>\n",
       "      <td>161</td>\n",
       "      <td>189</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>F</td>\n",
       "      <td>170</td>\n",
       "      <td>58</td>\n",
       "      <td>38.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>bl</td>\n",
       "      <td>1</td>\n",
       "      <td>14.6</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0</td>\n",
       "      <td>vs</td>\n",
       "      <td>159</td>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>F</td>\n",
       "      <td>168</td>\n",
       "      <td>60</td>\n",
       "      <td>41.0</td>\n",
       "      <td>R</td>\n",
       "      <td>NS</td>\n",
       "      <td>bl</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3</td>\n",
       "      <td>ju</td>\n",
       "      <td>173</td>\n",
       "      <td>178</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>F</td>\n",
       "      <td>172</td>\n",
       "      <td>62</td>\n",
       "      <td>39.0</td>\n",
       "      <td>R</td>\n",
       "      <td>S</td>\n",
       "      <td>lb</td>\n",
       "      <td>4</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.8</td>\n",
       "      <td>1</td>\n",
       "      <td>ju</td>\n",
       "      <td>165</td>\n",
       "      <td>185</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  height  weight  shoe_size right_left_handed smoker_nonsmoker  \\\n",
       "0       M     183      72       42.0                 R               NS   \n",
       "1       M     183      68       43.0                 R               NS   \n",
       "2       M     182      73       44.0                 R               NS   \n",
       "3       M     175      66       41.0                 L               NS   \n",
       "4       M     158      42       41.0                 R               NS   \n",
       "5       M     179      70       47.0                 R                S   \n",
       "6       F     172      61       38.0                 L               NS   \n",
       "7       M     185      69       44.0                 R               NS   \n",
       "8       M     177      75       43.0                 R                S   \n",
       "9       F     165      50       37.0                 R               NS   \n",
       "10      M     186      93       45.0                 R                S   \n",
       "11      F     165      53       39.0                 R               NS   \n",
       "12      F     156      48       37.0                 R               NS   \n",
       "13      M     178      75       43.0                 R               NS   \n",
       "14      F     172      49       39.0                 R               NS   \n",
       "15      F     168      52       38.0                 R               NS   \n",
       "16      M     183      72       42.0                 R               NS   \n",
       "17      M     178      73       42.0                 R               NS   \n",
       "18      M     183      60       43.0                 R               NS   \n",
       "19      F     166      48       38.0                 R               NS   \n",
       "20      M     177      70       45.0                 R               NS   \n",
       "21      M     180      70       43.0                 R               NS   \n",
       "22      M     174      69       42.0                 R               NS   \n",
       "23      M     184      70       46.0                 R               NS   \n",
       "24      M     168      69       43.0                 R                S   \n",
       "25      F     176      59       39.0                 R               NS   \n",
       "26      F     159      51       38.0                 R                S   \n",
       "27      M     169      60       41.0                 R               NS   \n",
       "28      F     164      61       38.5                 R                S   \n",
       "29      F     150      49       35.0                 R               NS   \n",
       "30      F     163      54       37.5                 R               NS   \n",
       "31      M     181      96       44.0                 R               NS   \n",
       "32      F     169      68       40.0                 L               NS   \n",
       "33      M     170      65       44.0                 R                S   \n",
       "34      F     160      50       34.5                 R               NS   \n",
       "35      M     180      63       42.0                 R               NS   \n",
       "36      F     165      49       38.0                 R               NS   \n",
       "37      M     184      80       45.0                 R               NS   \n",
       "38      F     170      65       40.0                 R               NS   \n",
       "39      F     173      66       40.0                 R               NS   \n",
       "40      F     165      63       39.0                 L               NS   \n",
       "41      F     159      52       37.0                 R               NS   \n",
       "42      M     181      65       44.0                 L               NS   \n",
       "43      F     175      76       42.0                 R               NS   \n",
       "44      F     170      58       38.0                 R               NS   \n",
       "45      F     168      60       41.0                 R               NS   \n",
       "46      F     172      62       39.0                 R                S   \n",
       "\n",
       "   hair_colour eye_colour  R_wrist_girth  L_wrist_girth  nb_siblings  \\\n",
       "0           lb          1           17.0           16.6            2   \n",
       "1           db          2           17.0           16.0            1   \n",
       "2           bl          1           18.5           18.0            2   \n",
       "3           db          2           17.0           17.0            2   \n",
       "4           db          2           14.0           14.0            2   \n",
       "5           db          2           16.0           16.0            1   \n",
       "6           lb          1           15.3           15.0            2   \n",
       "7           db          2           16.5           15.0            1   \n",
       "8           db          2           17.0           16.3            1   \n",
       "9           db          3           15.0           15.0            3   \n",
       "10          db          2           18.6           18.7            1   \n",
       "11          lb          1           15.3           15.0            1   \n",
       "12          db          3           15.0           15.2            2   \n",
       "13          db          2           17.0           16.8            0   \n",
       "14          lb          2           14.0           14.0            1   \n",
       "15          lb          2           14.1           13.7            1   \n",
       "16          lb          1           17.0           16.6            2   \n",
       "17          db          2           18.0           17.0            1   \n",
       "18          db          3           17.0           17.0            1   \n",
       "19          lb          4           14.0           14.0            2   \n",
       "20          bl          3           17.0           17.0            1   \n",
       "21          db          2           17.0           17.0            4   \n",
       "22          db          2           16.7           16.4            1   \n",
       "23          db          2           17.0           17.5            2   \n",
       "24          db          2           16.5           16.0            1   \n",
       "25          bl          2           15.0           15.0            1   \n",
       "26          db          3           15.6           15.8            1   \n",
       "27          bl          1           15.0           15.0            1   \n",
       "28          bl          2           15.5           15.5            1   \n",
       "29          db          2           14.5           14.5            4   \n",
       "30          db          2           15.2           14.8            3   \n",
       "31          lb          1           18.6           19.3            1   \n",
       "32          bl          2           14.3           14.5            1   \n",
       "33          bl          2           15.2           15.5            1   \n",
       "34          db          2           14.5           14.3            0   \n",
       "35          bl          2           14.5           14.2            2   \n",
       "36          db          2           14.0           14.5            1   \n",
       "37          db          2           17.0           16.0            2   \n",
       "38          db          2           16.0           15.0            2   \n",
       "39          lb          2           16.5           16.0            1   \n",
       "40          lb          3           17.2           17.2            0   \n",
       "41          bl          3           15.0           15.0            1   \n",
       "42          db          2           17.5           17.0            1   \n",
       "43          db          2           16.6           16.8            1   \n",
       "44          bl          1           14.6           14.7            0   \n",
       "45          bl          1           14.5           14.0            3   \n",
       "46          lb          4           14.8           14.8            1   \n",
       "\n",
       "   birth_place  height_M  height_F  nb_siblings_F  nb_siblings_M diet  \n",
       "0           vd       170       174              5              1    4  \n",
       "1           fr       170       178              4              2    2  \n",
       "2           ne       168       181              3              5    4  \n",
       "3           vd       156       178              4              7    1  \n",
       "4           vd       156       178              4              7    1  \n",
       "5           vd       150       190              1              0    4  \n",
       "6           vd       168       186              2              4    2  \n",
       "7           vd       172       167              4              0    3  \n",
       "8           fr       161       169              3              2    3  \n",
       "9           vd       163       175              1              3    1  \n",
       "10          vd       176       173              0              2    4  \n",
       "11          ju       166       180              3              5    1  \n",
       "12          vd       168       177              1              1    2  \n",
       "13          vd       175       167              0              1    3  \n",
       "14          vd       168       180              6              4    3  \n",
       "15          vd       165       185              1              2    1  \n",
       "16          vd       170       174              5              1    2  \n",
       "17      bresil       165       168              7              2    4  \n",
       "18          vd       174       178              1              3    2  \n",
       "19          vd       158       172              1              2    1  \n",
       "20          vd       173       175              0              0    3  \n",
       "21          vd       160       170              7              6    1  \n",
       "22          vd       164       175              7              2    3  \n",
       "23          vd       160       175              2              0    4  \n",
       "24          vd       153       176             14              2    3  \n",
       "25          vd       166       182              1              1    4  \n",
       "26          fr       168       167              2              2    2  \n",
       "27          vs       157       168              4              3    1  \n",
       "28      france       170       178              1              6    3  \n",
       "29    colombie       148       165             13              1    3  \n",
       "30          ju       167       164              7              5    2  \n",
       "31          vd       164       174              2              0    4  \n",
       "32          ti       161       178              0              4    2  \n",
       "33          vd       175       178              1              0    2  \n",
       "34          vd       162       172              2              1    1  \n",
       "35          vs       160       182              2              2    2  \n",
       "36          ti       168       182              7              1    2  \n",
       "37          ti       162       175              4              5    3  \n",
       "38          vs       161       178              2              2    4  \n",
       "39          ge       175       185              1              1    3  \n",
       "40          zh       162       182              1              1    4  \n",
       "41          vd       157       170              2              2    1  \n",
       "42          vd       162       172              2              2    2  \n",
       "43          vd       161       189              3              3    4  \n",
       "44          vs       159       180              2              2    3  \n",
       "45          ju       173       178              2              1    2  \n",
       "46          ju       165       185              4              2    4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('data/etubiol.csv')\n",
    "df.rename(columns={i:i.replace('.','_') for i in df.columns},inplace=True)\n",
    "df.eye_colour = df.eye_colour.astype(str) # making eye colour a non-numerical variable\n",
    "df.diet = df.diet.astype(str) # making diet a non-numerical variable\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which we would like to model the height of individuals given the following informations : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_siblings_F\n",
      "weight\n",
      "height_F\n",
      "L_wrist_girth\n",
      "shoe_size\n",
      "nb_siblings_M\n",
      "height_M\n",
      "smoker_nonsmoker\n",
      "birth_place\n",
      "eye_colour\n",
      "R_wrist_girth\n",
      "right_left_handed\n",
      "diet\n",
      "hair_colour\n",
      "gender\n",
      "nb_siblings\n"
     ]
    }
   ],
   "source": [
    "for s in set(df.columns)-set(['height']):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.right_left_handed.dtype == 'O'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# 1. correlation   <a id='0'></a>\n",
    "\n",
    "Correlation is a measure of the amount of relatedness between two measured variables.\n",
    "\n",
    "A correlation measure typically goes from -1 (anti-correlation) to 1 (correlation), where 0 is the absence of correlation (independence).\n",
    "\n",
    "When two variables show a very large correlation, one can be said to be a **predictor** of the other, in the sense that knowing the value of one of the two variable allows us to make a reasonnable guess about the value of the second variable.\n",
    "Another way of looking at this relationship is to see those variable as **redundant**, in the sense that they carry the same information : knowing the value of both variable does not bring much more insight compared to knowing the value of only one.\n",
    "\n",
    "\n",
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.1. Pearson's (linear) correlation  <a id='1'></a>\n",
    "\n",
    "\n",
    "Given 1 sample where each individual $i$ has 2 measures $x_i$ and $y_i$, Pearson's correlation coefficient between $x$ and $y$ is : \n",
    "\n",
    "$$r_{x,y} = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2}\\sqrt{\\sum(y_i - \\bar{y})^2}}$$\n",
    "\n",
    "A way to look at this formula is that $r_{x,y}$ tends to move away from zeros when points for which $x_i$ is very different from its mean corresponds to points for which $y_i$ is also very different from its mean. Thus, we are looking for an association in the variation of the variables, which is why Pearson correlation coefficient is also defined as a **standardized covariance** of the two variables.\n",
    "\n",
    "Pearson's correlation coefficient measures the **linear correlation** between variables, which means that its value is only relevant for the evaluation of a linear relationship. In other words, **two variables can have a strong relationship (i.e. be correlated) but display a Pearson's coefficient of correlation equal to 0**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Correlation_examples2](images/Correlation_examples2.png)\n",
    "> Image by DenisBoigelot, released into the public domain (CC0 license)\n",
    "\n",
    "This emphasises the danger of relying on a single number for representing sometimes complex notions and the \n",
    "importance of always representing visually the variables you want to describe.\n",
    "\n",
    "Another (fun) example is the [datasaurus dozen](https://www.autodeskresearch.com/publications/samestats) :\n",
    "\n",
    "![DinoSequentialSmaller.gif](images/DinoSequentialSmaller.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.2. Spearman's (rank) correlation coefficient  <a id='2'></a>\n",
    "\n",
    "Spearman's correlation coefficient corresponds to Pearson's correlation coefficient, but on the **ranks** of observations rather than their values.\n",
    "\n",
    "Spearman correlation coefficient is used to describe the correlation between two variables when their relation is *monotonic* (i.e. it goes in a single direction: if it is increasing it is always increasing, it never goes down) but non linear (e.g. an exponential relationship)\n",
    "\n",
    "The formula to calculate the Spearman's rank correlation coefficients  between two random variables **X** and **Y** associated to n individual drawns is:\n",
    "\n",
    "$$\\rho=1-\\frac{\\sum d_{i}^{2}}{n^{3}-n}$$\n",
    "\n",
    "Where i is the individual number, n the number of individuals and $d_i$ defined as follow :\n",
    "\n",
    "$d_i=rank(x_i)-rank(y_i)$\n",
    "\n",
    "Where of course $x_i$ and $y_i$ are the realization of **X** and **Y** for individual i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to calculate pearson and spearman coeffcient using the scipy.stats library (in the following code abbreviating by stats) followed by either `pearsonr` or `spearmanr`. Both take two lists or arrays as a input and return an array comprised of the coefficient and the p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=1./5\n",
    "\n",
    "linear=[[u,(u)/100+sigma*np.random.randn()] for u in range(10,500)]\n",
    "monotonic=[[u,50*(0.8**(u/10))+sigma*np.random.randn()] for u in range(10,500)]\n",
    "\n",
    "non_monotonic=[[u,(u)**3+3*u**2+sigma*np.random.randn()] for u in np.arange(-1,1,1./250)]\n",
    "\n",
    "together=[linear,monotonic,non_monotonic]\n",
    "plt.subplots(133,figsize=(15,5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    x=[u[0] for u in together[i]]\n",
    "    y=[u[1] for u in together[i]]\n",
    "    plt.scatter(x,y)\n",
    "    plt.title('Pearson: {0:.3f}, Spearman: {1:.3f}'.format(\n",
    "                                    stats.pearsonr(x,y)[0],##just like that\n",
    "                                    stats.spearmanr(x,y)[0]))\n",
    "plt.tight_layout()    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our real dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df['shoe_size']\n",
    "y=df['height']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Pearson: {0:.3f}, Spearman: {1:.3f}'.format(\n",
    "                                    stats.pearsonr(x,y)[0],##just like that\n",
    "                                    stats.spearmanr(x,y)[0]))\n",
    "plt.xlabel('Shoe size')\n",
    "plt.ylabel('Height')\n",
    "plt.tight_layout()    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.3. Significance of Pearson and Spearman correlation coefficient.  <a id='3'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 main ways to evaluate the significance of $\\rho$ (the coefficient of correlation) compared to zero. The most straighforward rely on a t-test to evaluate if $\\rho$ is significantly different from 0.\n",
    "\n",
    "Following a permutation  argument (so looking at a null model where you break data correlation/class by creating all the possible arrangements of your data), you can rationalize the usage of the following test statistic :\n",
    "\n",
    "$$t=\\rho\\sqrt{\\frac{n-2}{1-\\rho^2}}$$\n",
    "\n",
    "which follow a Student's t-distribution under the null hypothesis that $\\rho=0$.\n",
    "\n",
    "\n",
    "The other two ways to evaluate the significance of $\\rho$ are :\n",
    "\n",
    "- Do the permutation test yourself\n",
    "- Transform the data using Fisher transformation ($F(\\rho)=arctanh(\\rho)$) to calculate a z variable (instead of a t-student variable), which is normally distributed under $H_0$ : $z=\\sqrt{\\frac{n-3}{1.06}}F(\\rho)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.4 Kendall tau correlation coefficient (for fun)  <a id='4'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already seen that you could define a correlation coefficient in multiple manners using multiple metrics : multiplicative distances to the means for Pearson, ranking distances between the two random variables for Spearman. For completness let's rapidly present another way to measure correlation : Kendall tau.\n",
    "\n",
    "Kendal tau is based on the concept of concordant or discordant pairs. A concordant pair is a pair of individual i and j, $i<j$, for which the order relation between the two random variables stands : either $x_i>x_j$ and $y_i>y_j$ or $x_i<x_j$ and $y_i<y_j$. Discordant pairs have opposite signs between x and y.\n",
    "\n",
    "The coefficient is defined as follow:\n",
    "\n",
    "$$\\tau=\\frac{\\text{number of concordant pairs}-\\text{number of discordant pairs}}{\\frac{n(n-1)}{2}}$$\n",
    "\n",
    "Obviously this coefficient is comprised between -1 and 1, 0 meaning no correlation (indeed there is a total of $\\frac{n(n-1)}{2}$ unique pairs).\n",
    "\n",
    "A rank version also exists (which is mathematically equivalent to the expression above):\n",
    "\n",
    "$$\\tau_{rank}=\\frac{2}{n(n-1)}\\sum_{i<j} sgn(x_i-x_j)sgn(y_i-y_j)$$\n",
    "\n",
    "Both the Spearman and and Pearson correlation coefficent are relying on distances, which means they are sensitive to the intensity of the error term and to outliers.  Kendall tau is way less influenced by the scale of the variability since it relies only on an order relation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(133,figsize=(15,5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    x=[u[0] for u in together[i]]\n",
    "    y=[u[1] for u in together[i]]\n",
    "    plt.scatter(x,y)\n",
    "    plt.title('Kendall_tau: {0:.3f}, Kendall_tau_rank: {1:.3f}'.format(\n",
    "                            stats.kendalltau(x,y)[0],\n",
    "                            stats.mstats.kendalltau(x,y)[0]))\n",
    "plt.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise 01  <a id='5'></a>\n",
    "\n",
    "Given the *etubiol* dataset, rank the best correlators of *height*.\n",
    "\n",
    "Among these, which seem redundant to you (because of a high correlation)?\n",
    "\n",
    "> If you work with a panda dataframe you can use (or not) the functionality `DataFrame.corr()` to calculate the pearson correlation between all the variable of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_04_corr.py\n",
    "\n",
    "#sns.pairplot(df)\n",
    "\n",
    "corre=df.corr()\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(corre,cmap='plasma')\n",
    "plt.title('correlations')\n",
    "plt.show()\n",
    "\n",
    "sns.violinplot(y='gender',x='height',data=df , color=\"0.8\" )\n",
    "sns.stripplot(y='gender',x='height',data=df , zorder=1 )\n",
    "plt.show()\n",
    "\n",
    "sns.violinplot(y='smoker_nonsmoker',x='height',data=df , color=\"0.8\" )\n",
    "sns.stripplot(y='smoker_nonsmoker',x='height',data=df , zorder=1 )\n",
    "plt.show()\n",
    "\n",
    "#Not normal (do the normality test if you want). So no t test. Let's go for non parametric\n",
    "\n",
    "stat , pval =  stats.mannwhitneyu( df.height[df.gender=='M'] , \n",
    "                               df.height[df.gender=='F']  )\n",
    "print('Mann-Whitney rank test p-value for gender :' , pval)\n",
    "\n",
    "\n",
    "stat , pval =  stats.mannwhitneyu( df.height[df.smoker_nonsmoker=='NS'] , \n",
    "                               df.height[df.smoker_nonsmoker=='S']  )\n",
    "print('Mann-Whitney rank test p-value for smoker :' , pval)\n",
    "\n",
    "sns.violinplot(y='birth_place',x='height',data=df , color=\"0.8\" )\n",
    "sns.stripplot(y='birth_place',x='height',data=df , zorder=1 )\n",
    "plt.show()\n",
    "\n",
    "#Under represented labels : not a good feature\n",
    "\n",
    "sns.violinplot(y='hair_colour',x='height',data=df , color=\"0.8\" )\n",
    "sns.stripplot(y='hair_colour',x='height',data=df , zorder=1 )\n",
    "plt.show()\n",
    "\n",
    "sns.violinplot(y='eye_colour',x='height',data=df , color=\"0.8\" )\n",
    "sns.stripplot(y='eye_colour',x='height',data=df , zorder=1 )\n",
    "plt.show()\n",
    "\n",
    "sns.violinplot(y='diet',x='height',data=df , color=\"0.8\" )\n",
    "sns.stripplot(y='diet',x='height',data=df , zorder=1 )\n",
    "plt.show()\n",
    "\n",
    "# Again, no ANOVA for us here, so we replace it with a Kruskal-Wallis test. \n",
    "# H1 is a significant association of the factor with a change in the average of the numerical variable\n",
    "\n",
    "print('Kruskal test for hair colour')\n",
    "s,pval = stats.kruskal(df.height[df.hair_colour=='lb'] , df.height[df.hair_colour=='db'], df.height[df.hair_colour=='bl'])\n",
    "print('\\t\\t->',pval)\n",
    "\n",
    "print('Kruskal test for eye colour')\n",
    "s,pval = stats.kruskal(df.height[df.eye_colour=='1'] , df.height[df.eye_colour=='2'], df.height[df.eye_colour=='3'], df.height[df.eye_colour=='4'])\n",
    "print('\\t\\t->',pval)\n",
    "\n",
    "print('Kruskal test for diet')\n",
    "s,pval = stats.kruskal(df.height[df.diet=='1'] , df.height[df.diet=='2'], df.height[df.diet=='3'], df.height[df.diet=='4'])\n",
    "print('\\t\\t->',pval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_04_more.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.5 Correlation and causation  <a id='6'></a>\n",
    "\n",
    "\"Correlation does not equal causation\" is one of these [often](https://xkcd.com/552/) [repeated](https://www.explainxkcd.com/wiki/index.php/925:_Cell_Phones) [sentence](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation), but it is still true nonetheless.\n",
    "\n",
    "Observing that A and B are correlated (linearly or otherwise) gives you **no information** about whether A causes B or B causes A. One may not cause the other at all (they might both be caused by another [unidentified process](https://xkcd.com/1138/)!).\n",
    "\n",
    "Furthermore, even if a coefficient of correlation if very high, it might still be completely [spurious](https://www.tylervigen.com/spurious-correlations).\n",
    "\n",
    "![spurious](images/spurious_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is thus important to always shed a critical eye on correlations and the conclusion we could be tempted to draw from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# 2.Linear regression  <a id='7'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined correlation, we can see further ways to characterise the relationship between two variables.\n",
    "\n",
    "The problem we are interested in is the following:\n",
    "\n",
    "We measured a bunch of variables per individual, for many individuals. We are interested in the relationship between one of this variable that we will call the *response variable* ($Y$) and the other variables that we will call *covariables* ($X$). \n",
    "Of course our measurments are not perfect so there is some noise associated to it ($\\epsilon$). In mathematical term we are interested in a class of problem that we can write as :\n",
    "\n",
    "$\\pmb{Y}=f(\\pmb{X})+\\epsilon$\n",
    "\n",
    "The function $f$ is called the regression function, and today we will be interested in looking at a particular form of those function: **linear combination**.\n",
    "\n",
    "A particular case of linear combination would be a single covariable with an intercept like :\n",
    "\n",
    "$y_i=\\beta x_i+c$\n",
    "\n",
    "![linear_model1.png](images/linear_model1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more general case would have more covariables and would be written like:\n",
    "\n",
    "$$f(\\textbf{X}_i,\\pmb{\\beta})=\\sum_{p} \\beta_p x_{i,p}= \\textbf{X}_{i}^{T}\\pmb{\\beta}$$\n",
    "\n",
    "Where *$X_i$* is a vector of p covariables associated to point individual i.\n",
    "\n",
    "Note that for now nothing is said about the nature of the $x_{i,p}$, for example some could be constant instead of being a variable and thus you could go back to a more specific affine function (like $\\beta x+c$).\n",
    "\n",
    "So of course now the game become to best choose the vector of parameters $\\pmb{\\beta}$. For that there are two main methods (sorry Bayesian people...):\n",
    "- Least Square fit\n",
    "- Maximum Likelihood\n",
    "\n",
    "We will discuss both those methods. Least square fit is the most intuitive and easy to get a hold on, so hopefully you will leave this class with a rather good understanding of it. Maximum likelihood is a bit more advanced in terms of the concepts it utilizes, but being introduce to it will allow you to manipulate cool concepts that you will need by the end of this notebook and if you keep learning about statistics in general. \n",
    "\n",
    "Underlying those different methods, there are different models:\n",
    "\n",
    "- Linear models\n",
    "- Generalized linear models\n",
    "\n",
    "The way we wrote the function linking $Y$ to $X$ above, have the noise term $\\epsilon$ outside of the function. So one would say that this function only try to represent the mean of the response variable $Y$ along the curve, and as importantly, it does it looking at linear function. \n",
    ". This is what we actually do in the framework of Linear models : we only aim to fit the mean response using linear funcitons.\n",
    "\n",
    "Generalized linear model, in another hand, are more flexible : they allow us to transform the mean response and to fit that transformed response with a linear model. It is very powerfull, as now we could better modeled response variable with broader properties (count data, catgorical data etc....), but significantly more complicated and so we will not talk about those methods here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.1.Presentation  <a id='8'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarity let's define once for all some variables : we have a sample of size n, for each individual on this sample there are p+1 measurments, p covariables and one response variable. \n",
    "\n",
    "In the least square method we are interested in making the smallest overall distance error between our model and the response variable. \n",
    "Typically we want to find the $\\beta$ that minimizes:\n",
    "\n",
    "$S(\\pmb\\beta)=\\sum_i (y_i-f(\\textbf{X},\\pmb{\\beta}))^2=\\sum_i \\epsilon_i^2$\n",
    "\n",
    "in mathematical terms you are looking for \n",
    "\n",
    "$\\hat{\\pmb\\beta}=\\text{arg min}_{\\pmb\\beta}S(\\pmb\\beta)$\n",
    "\n",
    "Here the sum is over i, which counts the number of individuals.\n",
    "\n",
    "> The hat $\\hat{.}$, is a notation we use to denote our estimate of the true value of something. So in that sense $\\hat{\\pmb\\beta}$ is the estimate of the \"real\" coefficient values, and $\\hat{Y}$ is the estimation of $Y$ given by our model (also called the model predictions).\n",
    "\n",
    "Let's try to represent this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a real case scenario just to show case the outcome of what we are trying to do ultimately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we want to end up with thefollowing graph and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df['shoe_size']\n",
    "y=df['height']\n",
    "\n",
    "\n",
    "plt.scatter(x,y,label='data')\n",
    "slope , intercept , r , pval , stderr = stats.linregress(x,y)#linear regression explaining disease progression thanks \n",
    "#to bmi\n",
    "\n",
    "print(r\"slope also called beta in our notation= \",slope)\n",
    "print(\"intercept also called c in our notation= \",intercept)\n",
    "\n",
    "\n",
    "yPredict = x * slope + intercept #now that we have the outcome iof the regression which is in this case a slope and\n",
    "#an intercept we can calulate what the model will predict as a diseas progression given a bmi\n",
    "plt.plot( x , yPredict , color = 'red',label='model')#the outcome of the regression is this red line\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('shoe_size')\n",
    "plt.ylabel('height')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does Least square method do to end up with this line? Well let's see it on some simple mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import collections  as mc\n",
    "\n",
    "## let's create some data to plot\n",
    "slopeReal = 3\n",
    "noise = 3\n",
    "x = np.arange(10)\n",
    "y = slopeReal * x + noise * np.random.randn(len(x)) # y = beta * x + some noise (no intercept here)\n",
    "## alternatively we could have:\n",
    "# x= df['shoe_size'] \n",
    "# y= df['height']\n",
    "## although note that in that case we also need an intercept. you can try 70\n",
    "\n",
    "\n",
    "\n",
    "# The challenge of least square regression is to find the slope that minimizes the squared error\n",
    "# let's try two possible values for the slope \n",
    "estimatedSlopes = [1,2.5]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(estimatedSlopes) , figsize = (14,7))\n",
    "\n",
    "for i,slopeEstimate in enumerate(estimatedSlopes):\n",
    "    yPredicted = slopeEstimate * x # prediction of y given the estimated slope and values of x\n",
    "\n",
    "    # error of the prediction\n",
    "    predictionSquaredError = sum( ( yPredicted - y )**2 )\n",
    "\n",
    "    ax[i].plot(x,y, 'o')\n",
    "    ax[i].plot(x,yPredicted, color='orange' , linewidth=2)\n",
    "\n",
    "    # now, let's represent the fitting error as segments between real and estimated values\n",
    "    Real = [i for i in zip(x,y)]\n",
    "    Predicted = [i for i in zip(x,yPredicted)]\n",
    "    lc = mc.LineCollection(zip(Real,Predicted) , colors='black')\n",
    "    ax[i].add_collection(lc)\n",
    "\n",
    "    ax[i].set_title('slope : {} - squared error : {:.2f}'.format(slopeEstimate,predictionSquaredError) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the best value for the slope, we could try a lot of them :\n",
    "possibleSlopes = np.linspace(0,6,101)\n",
    "print('all the slopes tested',possibleSlopes)\n",
    "errors = []\n",
    "for sl in possibleSlopes: # we compute the sum of squared error for each slopes\n",
    "    yPred = sl*x\n",
    "    errors.append( sum( yPred - y )**2 )\n",
    "\n",
    "plt.plot(possibleSlopes , errors )\n",
    "plt.xlabel('estimated slope')\n",
    "plt.ylabel('sum of squared errors')\n",
    "print( 'slope estimate with the smallest error : ', possibleSlopes[np.argmin(errors)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While we could use various optimization algorithms to find the best value for $\\beta$, \n",
    "when the system is overdetermined (*i.e.*, you have more points than coefficients $\\beta_i$) an analytical solution exists. It is of the form:\n",
    "\n",
    "$$\\hat{\\pmb\\beta}=(\\pmb X^T \\pmb X)^{-1}\\pmb X^T \\pmb Y$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.2.Underlying hypothesis  <a id='9'></a>\n",
    "\n",
    "There are a couple of important hypothesis behind this method:\n",
    "\n",
    "- **Correct specification** : have a good incentive for the function you use\n",
    "- **Strict exogeneity** : the errors are centered around the true value of y\n",
    "- **No linear dependance** : you can not reconstruct one of your covariable by summing a subset of your covariables with some set of constant weights \n",
    "- **Spherical errors**: \n",
    "    - Homoscedasticity : the spread of the error is the same along the curve (for example not true for counts data).\n",
    "    - No autocorrelation : error are not correlated along the curve.\n",
    "\n",
    "The linear dependance part has to do with the part of the exercise where I clustered highly correlated covariables together. If you want to produce a good model for prediction then be carefull about that point. You can have a feeling of what the problem is by imagining that 2 covariables are actually copy pasta of each other : there is not unique way to associate a weight to them... Also then you have 1 variable which bring nothing new to the modeling... so kind of worthless. This is why later on I will ask you to work on a restricted part of the covariables.\n",
    "\n",
    "If your goal is not really to produce a predictive model but more to infer the size effect of some covariables on your target variable, then it is not too crucial. Just remember that if this is what you want there are other steps to take, that are far beyond the scope of this course, and which are related to the field of causal inference\n",
    "\n",
    "Normality is not strictly needed for Least Square fitting, neither for the variables nor for their errors. \n",
    "However you may need that hypothesis downstream in your analysis, for instance when using a test statistic.\n",
    "\n",
    "If you errors are normally distributed, then Least Square fitting and Maximum Likelihood are equivalent, showing that your method for choosing $\\pmb\\beta$ is efficient and sound.\n",
    "\n",
    "We will quickly present the Maximum Likelihood equivalent as it is both a very useful technic and helps broadening linear models to Generalized Linear Models.\n",
    "\n",
    "Finally, within that set of constraints and even if the method is called Linear Models, it is possible to fit polynomials of a degree bigger than 1. To do so you just have to precompute the monomials and add them to your set of covariables.\n",
    "\n",
    "For example :\n",
    "\n",
    "$y=\\beta x +c$ is a linear combination of x\n",
    "\n",
    "$y=\\beta_{1}x+\\beta_{2}x^{2}+\\beta_{3}x^{3}$ is still a linear combination of features (covariables) x, $x^{2}$ and $x^{3}$, and **X** becomes {$x,x^2,x^3$\\}\n",
    "\n",
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.3. Goodness of fit  <a id='10'></a>\n",
    "\n",
    "To have an idea of how good your fit is, you can either directly use the Mean Squared Error (MSE) or the adjusted coefficient of determination $\\pmb R^2_a$.\n",
    "\n",
    "The MSE is defined as follow:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$MSE=\\frac{\\sum (y_i-\\hat{y_i})^2}{n-2}$$ \n",
    "and accounts for what your model is missing. \n",
    "That could be the simple inherent variance induced by the noise term or the noise term and a missing term that your model doesn't take into account. By its nature, this metric makes it hard to compare between different hypothetical fitting models or different dataset.\n",
    "\n",
    "A better normalized metric is the **adjusted coefficient of determination $\\pmb R^2_a$**. \n",
    "The adjusted part is very necessary when we work in the context of multiple linear regression (more than one covariable). \n",
    "\n",
    "Let's start by defining the coefficient of determination $\\pmb R^2$. \n",
    "This coefficient partitions the variance present in your data between what is taken into account by your model and what is not.\n",
    "\n",
    "$$R^2=1-\\frac{SSE}{SST}$$, where SSE is the sum of squared errors ($\\sum_i (y_i-\\hat{y_i})^2$) and SST in the sum of squares total ($\\sum_i (y_i-\\bar{y})^2$)\n",
    "\n",
    "For the adjusted coefficient of determination you have to take into account that SSE and SST don't have the same degree of freedom and you should adjust for that.\n",
    "\n",
    "$$R^2_a=1-\\frac{n-1}{n-p}(1-R^2)$$, with $p$ the number of covariables and $n$ the number of individuals.\n",
    "\n",
    "> Note : you can see that when there is only one covariable then $R^2_a = R^2$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples:  some linear regression examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is typically the case where we would like to describe height = $\\beta$ shoe_size + c\n",
    "\n",
    "Here we look at a model y=1+3*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X=np.array(np.arange(-1,1,10**-2))\n",
    "fig,ax=plt.subplots(2,3,figsize=(15,10))\n",
    "\n",
    "for k,epsilon in enumerate([0,1,10]):\n",
    "    \n",
    "    y = 1+3*X + epsilon* np.random.randn(  len(X) )\n",
    "    \n",
    "    ## creating a dataframe with the data\n",
    "    X1 = sm.add_constant(X)##adding the intercept    \n",
    "    df_=pd.DataFrame(X1,columns=['c','x'])\n",
    "        \n",
    "    model = sm.OLS( y , df_[['c','x']])##defining an Ordinary Least Square variable\n",
    "    results = model.fit()##fitting it\n",
    "    y_predict=results.predict(X1)# predict back what your target variable would be in that model\n",
    "    R2=r2_score(y,y_predict)#evaluate R2\n",
    "    MSE=mean_squared_error(y,y_predict)#evaluate MSE\n",
    "\n",
    "    ## plotting the data and model\n",
    "    ax[0,k].plot(X,y,'ko',label='Data',linewidth=10,alpha=0.5)\n",
    "    ax[0,k].plot(X,y_predict,'r-.',label='Predicted')\n",
    "    ax[0,k].legend(loc='best',fontsize=10)\n",
    "    ax[0,k].set_title('R2={0:.2f}, MSE={1:.2f}, noise={2}'.format(R2,MSE,epsilon))\n",
    "    ax[0,k].set_xlabel('X')\n",
    "    ax[0,k].set_ylabel('y')\n",
    "    \n",
    "    ## plotting predicted value versus real value is a good way to visualize a fit\n",
    "    ax[1,k].plot(y,y_predict,'ko')\n",
    "    ax[1,k].set_xlabel('true y')\n",
    "    ax[1,k].set_ylabel('predicted y')\n",
    "    print('epsilon',epsilon)\n",
    "    print('fit param c= {0:.3f} beta= {1:.3f}'.format(results.params['c'],results.params['x']))\n",
    "    print('true param c= {0:.3f} beta= {1:.3f}'.format(1,3))\n",
    "    print()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happened if we miss specify the polynomial degree :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(np.arange(-1,1,10**-2))\n",
    "fig,ax=plt.subplots(2,3,figsize=(15,10))\n",
    "\n",
    "for k,epsilon in enumerate([0,1,10]):\n",
    "\n",
    "    y = 1+3*X + epsilon* np.random.randn(  len(X) )\n",
    "    \n",
    "    X1 = np.column_stack((X, X**2,X**3))\n",
    "    X1 = sm.add_constant(X1)##adding the intercept\n",
    "    \n",
    "    df_=pd.DataFrame(X1,columns=['c','x','x²','x³'])\n",
    "    \n",
    "    model = sm.OLS( y , df_[['c','x','x²','x³']])##defining an Ordinary Least Square variable\n",
    "    results = model.fit()##fitting it\n",
    "    y_predict=results.predict(X1)# predict back what your target variable would be in that model\n",
    "    R2=r2_score(y,y_predict)#evaluate R2\n",
    "    MSE=mean_squared_error(y,y_predict)#evaluate MSE\n",
    "\n",
    "\n",
    "    ax[0,k].plot(X,y,'ko',label='Data',linewidth=10,alpha=0.5)\n",
    "    ax[0,k].plot(X,y_predict,'r-.',label='Predicted')\n",
    "    ax[0,k].legend(loc='best',fontsize=10)\n",
    "    ax[0,k].set_title('R2={0:.2f}, MSE={1:.2f}, noise={2}'.format(R2,MSE,epsilon))\n",
    "    ax[0,k].set_xlabel('X')\n",
    "    ax[0,k].set_ylabel('y')\n",
    "    \n",
    "    ax[1,k].plot(y,y_predict,'ko')\n",
    "    ax[1,k].set_xlabel('true y')\n",
    "    ax[1,k].set_ylabel('predicted y')\n",
    "    print('epsilon',epsilon)\n",
    "    print('fit param c= {0:.3f} beta1= {1:.3f} beta2= {2:.3f} beta3= {3:.3f}'.format(results.params['c'],results.params['x'],results.params['x²'],results.params['x³']))\n",
    "    print('true param c= {0:.3f} beta= {1:.3f}'.format(1,3))\n",
    "    print()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the height datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "model = smf.ols(formula='height ~ shoe_size', data=df)\n",
    "\n",
    "results = model.fit()#we do the actual fit\n",
    "y_predict=results.predict(df)# predict back what your target variable would be in that model\n",
    "R2=r2_score(df['height'],y_predict)\n",
    "MSE=mean_squared_error(df['height'],y_predict)\n",
    "\n",
    "\n",
    "plt.plot(df['shoe_size'],df['height'],'ko',label='Data',linewidth=10,alpha=0.5)\n",
    "plt.plot(df['shoe_size'],y_predict,'r-.',label='Predicted')\n",
    "plt.legend(loc='best',fontsize=10)\n",
    "plt.title('R2={0:.2f}, MSE={1:.2f}'.format(R2,MSE))\n",
    "plt.xlabel('shoe_size')\n",
    "plt.ylabel('height')\n",
    "plt.show()  \n",
    "\n",
    "plt.plot(df['height'],y_predict,'ko')\n",
    "plt.xlabel('true height')\n",
    "plt.ylabel('predicted height')\n",
    "plt.show()\n",
    "print('fit param for shoe_size ',results.params['Intercept'])\n",
    "print('fit intercept ', results.params['shoe_size'])\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unidimensional, multiple covariables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is typically the case where we would like to describe $\\text{height}$ = $\\beta_1$ $shoesize$ +$\\beta_2$ $shoesize^2$ +$\\beta_3$ $shoesize^3$+c\n",
    "\n",
    "\n",
    "Here we look at a model y=1-3*x+6*x^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(np.arange(-1,1,10**-2))\n",
    "fig,ax=plt.subplots(2,3,figsize=(15,10))\n",
    "\n",
    "for k,epsilon in enumerate([0,1,10]):\n",
    "    y = 1-3*X+6*X**3 +epsilon*np.random.randn(len(X))\n",
    "\n",
    "    \n",
    "    X1 = np.column_stack((X, X**2,X**3))\n",
    "    X1 = sm.add_constant(X1)##adding the intercept\n",
    "    \n",
    "    df_=pd.DataFrame(X1,columns=['c','x','x²','x³'])\n",
    "    \n",
    "    model = sm.OLS( y , df_[['c','x','x²','x³']])##defining an Ordinary Least Square variable\n",
    "    results = model.fit()##fitting it\n",
    "    y_predict=results.predict(X1)# predict back what your target variable would be in that model\n",
    "    R2=r2_score(y,y_predict)#evaluate R2\n",
    "    MSE=mean_squared_error(y,y_predict)#evaluate MSE\n",
    "\n",
    "\n",
    "    ax[0,k].plot(X,y,'ko',label='Data',linewidth=10,alpha=0.5)\n",
    "    ax[0,k].plot(X,y_predict,'r-.',label='Predicted')\n",
    "    ax[0,k].legend(loc='best',fontsize=10)\n",
    "    ax[0,k].set_title('R2={0:.2f}, MSE={1:.2f}, noise={2}'.format(R2,MSE,epsilon))\n",
    "    ax[0,k].set_xlabel('X')\n",
    "    ax[0,k].set_ylabel('y')\n",
    "    \n",
    "    ax[1,k].plot(y,y_predict,'ko')\n",
    "    ax[1,k].set_xlabel('true y')\n",
    "    ax[1,k].set_ylabel('predicted y')\n",
    "    print('epsilon',epsilon)\n",
    "    print('fit param c= {0:.3f} beta1= {1:.3f} beta2= {2:.3f} beta3= {3:.3f}'.format(results.params['c'],results.params['x'],results.params['x²'],results.params['x³']))\n",
    "    print('true param c= {0:.3f} beta1= {1:.3f} beta2= {2:.3f} beta3= {3:.3f}'.format(1,-3,0,6))\n",
    "    print()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the real data, eventhough you probably have no reason to model it with something else than a degree 1 polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extended=df.copy()\n",
    "df_extended['shoe_size2']=df['shoe_size']**2\n",
    "df_extended['shoe_size3']=df['shoe_size']**3\n",
    "model = smf.ols(formula='height ~ shoe_size + shoe_size2 + shoe_size3', data=df_extended)\n",
    "\n",
    "results = model.fit()#we do the actual fit\n",
    "y_predict=results.predict(df_extended)# predict back what your target variable would be in that model\n",
    "R2=r2_score(df_extended['height'],y_predict)\n",
    "MSE=mean_squared_error(df_extended['height'],y_predict)\n",
    "\n",
    "plt.plot(df_extended['shoe_size'] , df_extended['height'] ,'ko',label='Data',linewidth=10,alpha=0.5)\n",
    "plt.plot(df_extended['shoe_size'] , y_predict ,'ro',label='Predicted')\n",
    "plt.legend(loc='best',fontsize=10)\n",
    "plt.title('R2={0:.2f}, MSE={1:.2f}'.format(R2,MSE))\n",
    "plt.xlabel('shoe_size')\n",
    "plt.ylabel('height')\n",
    "plt.show()    \n",
    "\n",
    "plt.plot( df_extended['height'], y_predict,'ko')\n",
    "plt.xlabel('true height')\n",
    "plt.ylabel('predicted height')\n",
    "plt.show()\n",
    "print('fit param for shoe_size ',results.params['Intercept'])\n",
    "print('fit intercept ', results.params['shoe_size'])\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multidimensional "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see what that look like with our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(formula='height ~ shoe_size+height_M', data=df)\n",
    "\n",
    "results = model.fit()#we do the actual fit\n",
    "y_predict=results.predict(df)# predict back what your target variable would be in that model\n",
    "R2=r2_score(df['height'],y_predict)\n",
    "MSE=mean_squared_error(df['height'],y_predict)\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(df['shoe_size'], df['height_M'], df['height'], s=20, c='k',label='Data', depthshade=True)\n",
    "\n",
    "ax.scatter(df['shoe_size'], df['height_M'],y_predict,label='Predicted' ,color='m')\n",
    "plt.legend(loc='best',fontsize=10)\n",
    "plt.title('R2={0:.2f}, MSE={1:.2f}'.format(R2,MSE))\n",
    "plt.xlabel('shoe_size')\n",
    "plt.ylabel('height_M')\n",
    "ax.set_zlabel('height')\n",
    "plt.show()    \n",
    "\n",
    "plt.plot(df['height'],y_predict,'ko')\n",
    "plt.xlabel('true height')\n",
    "plt.ylabel('predicted height')\n",
    "plt.show()\n",
    "print('fit param for shoe_size ',results.params['shoe_size'])\n",
    "print('fit param for height_M ',results.params['height_M'])\n",
    "print('fit intercept ',results.params['Intercept'])\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats model gives you way more info than just a predicition and your fitted parameters. But to really use those info that we will see later on, we need to introduce some other stuff first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.4. Confidence interval and test statistics  <a id='11'></a>\n",
    "\n",
    "After your fitting, you would probably like to know the confidence interval for each of your estimated $\\beta$, as well as if they are truly necessary (significantly different from zero). \n",
    "For both **you can't truly do anything without making an hypothesis about the statistic of the noise** : here comes the part where assuming your noise to be normally distributed ($N(0,\\sigma^2)$) becomes important, but potentially wrong too.\n",
    "\n",
    "For the confidence interval, if you have an infinite amount of data, and your noise distribution is not heavytailed, you can show that the estimators are well described by a normal statistic (there is convergence in the distribution so that $(\\hat{\\pmb\\beta}-\\pmb\\beta)\\rightarrow N(0,\\sigma^2 (\\pmb X^T \\pmb X)^{-1})$). \n",
    "So for big amount of points relative to the number of estimated parameters, you are not making a big mistake by writting:\n",
    "\n",
    "$$\\beta_p \\in [\\hat{\\beta_p} \\pm z_{1-\\frac{\\alpha}{2}}\\sqrt{\\hat{\\sigma}^2 [(\\pmb X^T \\pmb X)^{-1}]_{p,p}}]$$\n",
    "\n",
    "If you don't have a huge amount of data you need to show that you have an incentive about your noise statistic to use these kind of confidence intervals (some libraries that we are going to use can do that for you!).\n",
    "\n",
    "\n",
    "For the significance of the coefficients, **if you know that your noise is normally distributed then you can use a t-test**.\n",
    "\n",
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.5. Maximum Likelihood  <a id='12'></a>\n",
    "\n",
    "Maximum Likelihood is a method that is used to estimate parameters of a probablililty distribution, and is usefull for model choosing. It is done by maximizing the likelihood function. In the case that we are interested in (i.e. independant identically distributed) this likelihood function is simply the product of  a density function values over the entire sample. It is a parametric method since it needs to have an a priory about the density function for it to work. Since it is a product, most of the time we would rather work with the log likelihood function which transforms this product into a sum.\n",
    "\n",
    "So we would like to maximize $l$, the loglikelihood function, by choosing a set of parameters $\\Theta$.\n",
    "Where $l$ is of the form:\n",
    "\n",
    "$l(\\Theta;Y)=\\sum_i ln(p(y_i|\\Theta))$\n",
    "\n",
    "Where $Y$ is a random variable and $p()$ is the density function associated to $Y$.So you want to find the following estimation for $\\pmb\\Theta$\n",
    "\n",
    "$$\\hat{\\pmb\\Theta}=\\text{arg max}_{\\pmb\\Theta}l(\\pmb\\Theta;Y)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### What are we looking at?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the example of a gaussian where you would like to estimate the $\\sigma$ and the $\\mu$, given your data. As they are simulated data we chose that $\\mu=2$ and $\\sigma=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_dist(x,mu,sigma):\n",
    "    \"\"\" returns the probability of observing x in a normal distribution of mean mu and standard deviation sigma \"\"\"\n",
    "    return 1./(sigma*np.sqrt(2*np.pi))*np.exp(-1./(2*sigma**2)*(x-mu)**2)\n",
    "    # note : this is equivalent to stats.norm.pdf( x , mu , sigma )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small=np.random.randn(10)*0.5+2 # this is our observed data, with ( mean=2 , sd=0.5 )\n",
    "\n",
    "m=[2,0.5] # we will try 2 possible combinations of paramters ( mean=2 , sd=0.5 ) and ( mean=0.5 , sd=0.5 ) \n",
    "s=[0.5,0.5]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(m) , figsize = (14,7))\n",
    "X_small_=[[v,0] for v in X_small]\n",
    "\n",
    "x=np.arange(-2,4,0.005) # we will plot between -2 and 4\n",
    "\n",
    "print('the data that we observed',[v[0] for v in X_small_])\n",
    "\n",
    "for q in range(len(m)): # for each of the parameter combinations we want to try\n",
    "    ax[q].plot(X_small,[0]*len(X_small),'k+') # we plot the observed data as crosses\n",
    "\n",
    "    ax[q].plot( x , stats.norm.pdf( x , loc = m[q] , scale = s[q] ),'k') # we plot the distribution we are testing\n",
    "    \n",
    "    Predicted = stats.norm.pdf( X_small , loc = m[q] , scale = s[q] )\n",
    "\n",
    "    Predicted_= [i for i in zip(X_small,Predicted)] # this is to plot segments\n",
    "    lc = mc.LineCollection(zip(X_small_,Predicted_) , colors='red',linewidths=5,alpha=0.7,label='Predicted likelihood')\n",
    "    ax[q].add_collection(lc)\n",
    "    ax[q].legend(loc='best',fontsize=10)\n",
    "    \n",
    "    # the log likelihood of this set of parameters is the sum of the log of the probability densities of the sample\n",
    "    sum_like=sum(np.log(Predicted))     \n",
    "    ax[q].set_title('$\\mu$ : {} - $\\sigma$: {:.2f} - log likelihood : {:.2f}'.format(m[q],s[q],sum_like) ,fontsize=13)\n",
    "    \n",
    "    ax[q].set_xlabel('X')\n",
    "    ax[q].set_ylabel('Likelihood')\n",
    "\n",
    "\n",
    "plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying those red bars is exactly what the maximum likelihood does. \n",
    "\n",
    "Basically, you shift your theoritical distribution to the right or the left (trying different means), and you narrow it or widen it (trying different variances). \n",
    "\n",
    "For each of those try you multiply those red bars together, and the combination of parameters giving highest result is the one maximizing the likelihood of your data being produced by that distribution with those parameters.\n",
    "\n",
    "\n",
    "It is important to point out here that **even when our data are actually coming from a certain distribution, there will (almost) always be a difference between the theoretical distribution and the recovered one**, as to have perfect match you would need an infinite number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.randn(800)*0.5+2\n",
    "fig = plt.figure(figsize = (10,7)) \n",
    "sns.kdeplot(X,label='data probability\\ndensity function')\n",
    "x=np.arange(0,4,0.005)\n",
    "plt.plot(X,[0]*len(X) ,'k+',label='data')\n",
    "plt.plot(x, stats.norm.pdf( x , loc = 2 , scale = 0.5 ) ,'r',label='generative probability\\ndensity function')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.xlabel('X')\n",
    "plt.legend(loc='best',fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test many combinations of possible means and standard deviations to see where our maximum of likelihood lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "mu=np.arange(0,4,0.1) # from 0 to 4 by increments of 0.1\n",
    "sigma=np.arange(0.1,2.1,0.1) # from 0.1 to 2.1 by increments of 0.1\n",
    "\n",
    "mu,sigma=np.meshgrid(mu,sigma) # this useful function combines all possibles values for mu and sigma\n",
    "def loglike_func(X,mu,sigma):\n",
    "    \"\"\"returns a list of the loglikelihoods of mus and sigmas given data X\"\"\"\n",
    "    ll = []\n",
    "    for i in range(len(mu)):\n",
    "        ll.append( sum(np.log(stats.norm.pdf(X,mu[i],sigma[i]))) )\n",
    "        if math.isnan(ll[-1]) or ll[-1] < -10000: \n",
    "            ll[-1] = -10000 # we verify that no numerical error gave us an NaN or very small log value\n",
    "    return ll\n",
    "\n",
    "# we compute the log-likelihood for all tested parameters values \n",
    "zs=np.array(\n",
    "    loglike_func(X,np.ravel(mu),np.ravel(sigma))\n",
    ") \n",
    "loglike=zs.reshape(mu.shape)\n",
    "\n",
    "bestMu = np.ravel(mu)[np.argmax(zs)]\n",
    "bestSigma = np.ravel(sigma)[np.argmax(zs)]\n",
    "\n",
    "# make a 3D figure of our loglikelihood landscape\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "print(r'Highest likelihood is for \\mu and \\sigma :',bestMu,bestSigma)\n",
    "\n",
    "fig = plt.figure(figsize=(14,8)) \n",
    "ax = Axes3D(fig)\n",
    "ax.plot_surface(mu,sigma,loglike,cmap='plasma') \n",
    "ax.scatter(bestMu,bestSigma,max(zs),s=200,c='r') # put a dot at the ML value\n",
    "ax.set_xlabel('$\\mu$')\n",
    "ax.set_ylabel('$\\sigma$')\n",
    "ax.set_zlabel('Loglike')\n",
    "plt.title(\"Loglikelihood landscape\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the link between OLS and maximum likelihood (optional but a good gateway to understand GLM)\n",
    "\n",
    "Let's now imagine that we try to fit the average of a Y, $\\bar{Y}$, along the curve $\\bar{Y}=\\beta X+c$ for which the noise around those averages is gaussian. Since we didn't put the noise in this equality, thus it really represents a fit of the average of Y. The equation representing the fitting of Y would be $Y=\\beta X+c+\\epsilon$. We could thus consider that we can switch to the following problem of distribution fitting, defined by the density function:\n",
    "\n",
    "$$p(y_i|\\bar{y_i},\\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}*\\exp(-\\frac{1}{2}\\frac{(y_i-\\bar{y_i})^2}{\\sigma^2})$$\n",
    "\n",
    "Note that the parameters you want to estimate are $\\bar{y_i}$ and $\\sigma$.\n",
    "\n",
    "By definition of the  likelihood function over $n$ individuals in a sample is:\n",
    "\n",
    "$$\\Pi_i \\frac{1}{\\sqrt{2\\pi\\sigma^2}}*\\exp(-\\frac{1}{2}\\frac{(y_i-\\bar{y_i})^2}{\\sigma^2})$$\n",
    "\n",
    "which transformed into the loglikelihood function:\n",
    "\n",
    "$$l(\\bar{y_i},\\sigma;Y) = \\sum_i -\\frac{1}{2}\\frac{(y_i-\\bar{y_i})^2}{\\sigma^2} + constant$$\n",
    "\n",
    "Now let's rewrite $\\bar{y_i}=\\beta x_i+c=f(x_i,\\beta)$. So now the game is to find $\\beta$ and $c$.\n",
    "\n",
    "You see now that maximizing $\\sum_i -(y_i-f(x_i,\\beta))^2$ over $\\beta$ is the same than minimizing $\\sum_i (y_i-f(x_i,\\beta))^2$ over $\\beta$(which is what we wrote for Ordinary Least Square)\n",
    "\n",
    "\n",
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.6. Model choosing  <a id='13'></a>\n",
    "\n",
    "Most of the time you are not sure of the model you want to fit. You might have a broad idea of the different forms of the function but you don't really know for example what would be the best degree for your poynomial or if all the covariables are actually necessary. Of course you could say \"I am keeping the model that fit the best in term of $R^2$\". But the question really is : is that bunch of extra parameters that are complexifying my model worth the increase in $R^2$?\n",
    "\n",
    "We touched that question in 1.4 by asking about the significance of parameters values. Again if you are confident on the noise distribution you are dealing with (let's say it is normally distributed), and you have a function in mind but you don't know if you should include 1,2 or $p$ covariables then the problem is easy: you can use a log-likelihood ratio test.\n",
    "\n",
    "\n",
    "### Likelihood ratio test (LRT)\n",
    "\n",
    "\n",
    "To perform a likelihood ratio test you just have to calculate the difference between the maximised log-likelihood of the two models you are comparing. You can estimate the significance of that difference either by using a test statistic (approximate method) or by simulation.\n",
    "\n",
    "LRT are to be used in the case of nested function comparison. Nested functions are functions that have the same form but differ from the number of parameters used : for example comparing $y=\\beta_1 x_1 +c$ and $y=\\beta_1 x_1 +\\beta_2 x_2 +c$. In this course this will always be the case (but just remember that outside of this course you might want to do other comparison, so be carefull).\n",
    "\n",
    "Quickly :\n",
    "\n",
    "You want to compare model $M_0$ and $M_1$, respectively having $\\{\\beta_{1,0}\\}$ and $\\{\\beta_{1,2},\\beta_{2,2}\\}$ as parameters. You want to see if adding this extra parameter $\\beta_{2,2}$ is worth it.\n",
    "\n",
    "The LRT statistics is :\n",
    "\n",
    "$2*(l(Y;\\hat{\\beta}_{1,2},\\hat{\\beta}_{2,2},X)-l(Y;\\hat{\\beta}_{1,0},X))$\n",
    "\n",
    "Where the hat represents the maximum likelihood estimates. The LRT statistic asymptoptically, for your sample size going to infinity, follows a **chi-square distribution with a number of degree of freedom equal to the difference between the number of degrees of freedom in your models**. You have thus access to a P-value which will help you to decide if complexifying your model is worth it.\n",
    "\n",
    "To calulate this P-value you can use 1-scipy.stats.chi2.cdf(LRT,$df_{M_1}-df_{M_0}$), where $df$ is the number of degree of freedom of the models.\n",
    "\n",
    "\n",
    "### Regularization (for the culture)\n",
    "\n",
    "If you don't have access to the noise properties (*i.e.* you have no good reason to say it is normally distributed), you can always use a technic called regularization which is going to penalize covariables that are not really important to your fit. This is more on the machine learning side, and so a lot should be said about how to properly use  this technic (splitting your dataset between train, validation and test set, *etc.*). \n",
    "But let's just check what the principle behind it is and I will give an additionnal example on it later on.\n",
    "\n",
    "The only thing that this method does is to add a penalization term to the least square minimization method seen before. \n",
    "This penalization is based on the size of the parameters estimated. \n",
    "The rational is that some time, parameters estimated will be inflated to compensate the fact that the covariable is not really important to fit the data, but is rather important to understand the noise. So regularization minimizes square error while balancing the overall size of the parameters.\n",
    "\n",
    "Broadly, it can looks like that:\n",
    "\n",
    "* $S(\\pmb{\\beta}) + \\frac{1}{C}\\Sigma^{n}_{i=1}|\\beta_{i}|$ , l1 regularization (Lasso) C being the inverse of the weight that you put on that regularization \n",
    "\n",
    "* $S(\\pmb{\\beta}) + \\frac{1}{C}\\Sigma^{n}_{i=1}\\beta_{i}^{2}$ , l2 regularization (Ridge) \n",
    "\n",
    "* $S(\\pmb{\\beta}) + \\frac{1}{C}\\Sigma^{n}_{i=1}(\\alpha|\\beta_{i}|+(1-\\alpha)\\beta_{i}^{2})$ , elasticnet\n",
    "\n",
    "How to choose this C, or sometime $\\alpha$, is related to the field of machine learning and has to do with splitting your data set into train, validation and test sets. We will not go deeper than that but statsmodels has it implemented `statsmodels.regression.linear_model.OLS.fit_regularized` and scikitlearn, a python library specialized in machine learning has even more option. \n",
    "\n",
    "This is really just for culture, there are many more things to learn before applying those technics rigorously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples with Stats model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a mock dataset for which we know the ground truth : y=1-3x+6x^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statsmodel scale your variable to unit lenght automatically so no need for scaling here.\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "###making toy data\n",
    "nsample = 200\n",
    "x = np.linspace(0, 10, nsample)\n",
    "X = np.column_stack((x, x**3))\n",
    "beta = np.array([1, -3, 6])\n",
    "e = 1000*np.random.normal(size=nsample)#for now noise is 1000, but play with it\n",
    "\n",
    "X = sm.add_constant(X)##adding the intercept\n",
    "y = np.dot(X, beta) + e## making y=1-3x+6x^3 +noise\n",
    "y_true=np.dot(X, beta)\n",
    "df_=pd.DataFrame(X,columns=['c','x','x³'])\n",
    "df_['y']=y\n",
    "model = sm.OLS(df_['y'], df_[['c','x','x³']])##defining an Ordinary Least Square variable\n",
    "results = model.fit()##fitting it\n",
    "\n",
    "res=results.summary()\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first panel gives you an overview of the fit quality:\n",
    "* You recognize the good old $R^2$ and $R_a^2$\n",
    "* The F-statistic and its associated P-value test the hypothesis that all the coefficients are 0 (normality assumption)\n",
    "* You should also recognize the log-likelihood (normality assumption)\n",
    "* AIC and BIC respectively Aikike Information Criterion and Bayesian Information Criterion are equivalent of likelihood but that you can use to compare non nested models.\n",
    "\n",
    "The second panel is quite self explanatory, just be careful with this t-test which again makes the assumption that errors are normally distributed, same for the standard error and the 95% confidence interval.\n",
    "\n",
    "The third panel is a summary of a few statistical tests that will give you a sense of how all of the hypothesis needed for OLS are plausible:\n",
    "* Omnibus and Prob(omnibus): this is a test for normality of residuals. Low P-values means that your linear model is not adapted\n",
    "* Durbin-Watson : tests autocorrelation in the error terms (2 is no autocorrelation, less than 1 is bad)\n",
    "* Jarque-Bera: tests if the skewness and kurtosis of your errors are looking like a normal distribution. If the Pvalue is high then they look normal.\n",
    "* Condition Number : sensibility to noise of the fit.Skewness and kurtosis of your noise (both 0 for normally distributed noise).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Plotting the fit\n",
    "\n",
    "#for some noise: scale=1000\n",
    "\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "\n",
    "prstd, iv_l, iv_u = wls_prediction_std(results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.plot(x, y, 'o', label=\"data\")\n",
    "ax.plot(x, y_true, 'b-', label=\"True\")\n",
    "ax.plot(x, results.fittedvalues, 'r--.', label=\"OLS\")\n",
    "ax.plot(x, iv_u, 'r--')\n",
    "ax.plot(x, iv_l, 'r--')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### a little bit of gymnastic to get this summary saved and usable.\n",
    "\n",
    "results_as_html = res.tables[0].as_html()\n",
    "\n",
    "result_general_df2=pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "list1=[\"Dep. Variable:\"]+list(result_general_df2.index)+[result_general_df2.columns[1]]+list(result_general_df2[result_general_df2.columns[1]])\n",
    "list2=[result_general_df2.columns[0]]+list(result_general_df2[result_general_df2.columns[0]])+[result_general_df2.columns[2]]+list(result_general_df2[result_general_df2.columns[2]])\n",
    "\n",
    "dico_i={s:v for s,v in zip(list1,list2)}\n",
    "\n",
    "result_general_df=pd.DataFrame([[dico_i[v]] for v in list1],index=list1,columns=['Value']).transpose()\n",
    "\n",
    "\n",
    "results_as_html = res.tables[1].as_html()\n",
    "result_fit_df=pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "\n",
    "#print(result_general_df)\n",
    "#print(result_fit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have put the overall recap of the fit in a dataframe format so you can use it later\n",
    "\n",
    "result_general_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have put the outcome of the fit in a dataframe format so you can use it later\n",
    "result_fit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats model on etubiol dataset\n",
    "\n",
    "let's see how that work on real data: let's say we want to predict height using the height of the mother (`height_M`) and and shoe size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1 = smf.ols(formula='height ~ height_M + shoe_size', data=df)\n",
    "results_model1 = model1.fit()#we do the actual fit\n",
    "\n",
    "res=results_model1.summary()#we print the summary\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check with only shoe_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = smf.ols(formula='height ~ shoe_size', data=df)\n",
    "\n",
    "results_model2 = model2.fit()#we do the actual fit\n",
    "\n",
    "res=results_model2.summary()#we print the summary\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's add one more covariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loglikelihood model shoe_size:', results_model2.llf )\n",
    "print('loglikelihood model shoe_size + mother height:',results_model1.llf)\n",
    "#print('loglikelihood model shoe_size + mother height + number of siblings:',result_general_df_3['Log-Likelihood:']['Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already rule out number of siblings as it didn't change the loglikelihood. Adding weight did increase the loglikelihood, is it significant enought for us to keep it for modelling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRT=2*(results_model1.llf - results_model2.llf)\n",
    "print('The LRT statistics is ',LRT)\n",
    "print('The associated pvalue to that difference of Log likelihood is', 1-stats.chi2.cdf(LRT,2-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.7. What to do when some hypothesis about OLS are not true  <a id='14'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the homoscedasticity of your data is not true you have a few possibilities:\n",
    "- you can transform your data so your data become homoscedastic (for example you could use variance stabilizing transformation, or a simple log transform or other...)\n",
    "- you can change your loss function that we previously called $S(\\beta)$ to reweight the different members of that equation by taking into account the discrepancy in terms of variance. That only works if there is no correlation between the error terms. In that case the method is called Weighted Least Square and it simply transformed to $S(\\pmb\\beta)=\\sum_i \\frac{1}{\\sigma_i^2} (y_i-f(\\textbf{X},\\pmb{\\beta}))^2$.\n",
    "- if there is a correlation between the different error terms then it becomes more complicated, but technics exist such as Generalized Least Square model\n",
    "\n",
    "Finally if you know  what statistics your measurement follow, you can bypass all of those problems (and encounter others :-)) by using a maximum likelihood estimation rather than an LS method. By doing so you will have to put yourself in the framework of Generalized Linear Models, which is outside of the scope of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise 02  <a id='15'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following subset of covariables ['shoe_size','height_M','nb_siblings_F'] find the best model to predict height.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_04_reg.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final remark : to actually have the best model I invite you to follow the practice of machine learning that is based in splitting your dataset, cross validation etc... What you have learn today is still an introduction. You are more ready than ever to do modelisation but be aware that many things still need to be done to have you derive a model following state of the art methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
